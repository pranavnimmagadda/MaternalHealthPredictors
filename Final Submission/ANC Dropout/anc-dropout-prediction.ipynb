{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12069575,"sourceType":"datasetVersion","datasetId":7597211}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score\nimport time\nimport warnings\nimport pyarrow.parquet as pq\nwarnings.filterwarnings('ignore')\n\ndata_path = '/kaggle/input/fullfinal/telangana_data_with_features_and_targets (1).parquet'\nflag_map = {\n    'Y': 1, 'YES': 1, 'Yes': 1, 'y': 1, 'yes': 1,\n    'N': 0, 'NO': 0, 'No': 0, 'n': 0, 'no': 0,\n    None: np.nan, 'None': np.nan, '': np.nan, 'nan': np.nan\n}\n\ndef prepare_data_for_targets(data_path, flag_map, batch_size=10000):\n    \"\"\"Load and preprocess data from Parquet file in batches.\"\"\"\n    # Initialize an empty list to store processed chunks\n    processed_chunks = []\n    \n    # Define required and numeric columns\n    required_cols = ['MOTHER_ID', 'GRAVIDA']\n    numeric_cols = ['AGE', 'AGE_preg', 'AGE_final', 'GRAVIDA', 'PARITY', 'ABORTIONS', 'TOTAL_ANC_VISITS',\n                    'HEMOGLOBIN_mean', 'HEMOGLOBIN_min', 'WEIGHT_max', 'HEIGHT',\n                    'PHQ_SCORE_max', 'GAD_SCORE_max', 'WEIGHT_last', 'WEIGHT_first',\n                    'NO_OF_WEEKS_max', 'WEIGHT_min', 'WEIGHT_mean']\n    \n    # Open Parquet file using pyarrow\n    parquet_file = pq.ParquetFile(data_path)\n    num_rows = parquet_file.metadata.num_rows\n    num_batches = (num_rows + batch_size - 1) // batch_size  # Ceiling division\n    \n    # Iterate through batches\n    for batch_idx in range(num_batches):\n        # Read a batch of rows\n        start_row = batch_idx * batch_size\n        end_row = min(start_row + batch_size, num_rows)\n        batch = parquet_file.read_row_group(batch_idx).to_pandas() if parquet_file.num_row_groups > batch_idx else pd.DataFrame()\n        \n        if batch.empty:\n            continue\n            \n        # Check for required columns\n        for col in required_cols:\n            if col not in batch.columns:\n                raise ValueError(f\"Required column {col} missing in data\")\n\n        # Debug: Check non-numeric values\n        for col in numeric_cols:\n            if col in batch.columns and batch[col].dtype == 'object':\n                non_numeric = batch[col][~batch[col].apply(lambda x: str(x).replace('.', '').isdigit() if pd.notna(x) else False)]\n                if not non_numeric.empty:\n                    print(f\"Non-numeric values in {col}: {non_numeric.unique()[:10]}\")\n\n        # Clean GRAVIDA specifically\n        if 'GRAVIDA' in batch.columns:\n            batch['GRAVIDA'] = batch['GRAVIDA'].replace('nan', np.nan)\n            batch['GRAVIDA'] = pd.to_numeric(batch['GRAVIDA'], errors='coerce')\n            # Replace NaN with median GRAVIDA (or 1 as default) within batch\n            if batch['GRAVIDA'].notna().any():\n                median_gravida = batch['GRAVIDA'].median()\n                if pd.isna(median_gravida):\n                    median_gravida = 1.0\n                batch['GRAVIDA'] = batch['GRAVIDA'].fillna(0)\n                print(f\"Filled {batch['GRAVIDA'].isna().sum()} GRAVIDA NaN values with 0 in batch\")\n\n        # Convert other numeric columns\n        for col in numeric_cols:\n            if col in batch.columns and col != 'GRAVIDA':\n                batch[col] = pd.to_numeric(batch[col], errors='coerce')\n\n        # Map flag columns\n        if 'IS_CHILD_DEATH' in batch.columns and batch['IS_CHILD_DEATH'].dtype == 'object':\n            batch['IS_CHILD_DEATH'] = batch['IS_CHILD_DEATH'].map(flag_map)\n        if 'IS_DEFECTIVE_BIRTH' in batch.columns and batch['IS_DEFECTIVE_BIRTH'].dtype == 'object':\n            batch['IS_DEFECTIVE_BIRTH'] = batch['IS_DEFECTIVE_BIRTH'].map(flag_map)\n\n        # Fill NaN for numeric columns\n        batch_numeric_cols = batch.select_dtypes(include=['float64', 'float32', 'int64', 'int32', 'int8']).columns\n        batch[batch_numeric_cols] = batch[batch_numeric_cols].fillna(0)\n\n        # Append processed batch\n        processed_chunks.append(batch)\n\n    # Concatenate all chunks\n    df = pd.concat(processed_chunks, ignore_index=True)\n    return df\n\n# Execute the function\ndf = prepare_data_for_targets(data_path, flag_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T19:17:04.458769Z","iopub.execute_input":"2025-06-16T19:17:04.459211Z","iopub.status.idle":"2025-06-16T19:17:49.346024Z","shell.execute_reply.started":"2025-06-16T19:17:04.459180Z","shell.execute_reply":"2025-06-16T19:17:49.345076Z"}},"outputs":[{"name":"stdout","text":"Non-numeric values in GRAVIDA: ['nan']\nFilled 0 GRAVIDA NaN values with 0 in batch\nFilled 0 GRAVIDA NaN values with 0 in batch\nFilled 0 GRAVIDA NaN values with 0 in batch\nFilled 0 GRAVIDA NaN values with 0 in batch\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip show scikit-learn\n!pip show imbalanced-learn\n!pip install scikit-learn==1.2.2 imbalanced-learn==0.10.1 --no-deps -q\nfrom imblearn.over_sampling import SMOTE\nprint(\"SMOTE imported successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T09:58:16.166469Z","iopub.execute_input":"2025-06-17T09:58:16.166833Z","iopub.status.idle":"2025-06-17T09:58:26.067566Z","shell.execute_reply.started":"2025-06-17T09:58:16.166805Z","shell.execute_reply":"2025-06-17T09:58:26.066585Z"}},"outputs":[{"name":"stdout","text":"Name: scikit-learn\nVersion: 1.2.2\nSummary: A set of python modules for machine learning and data mining\nHome-page: http://scikit-learn.org\nAuthor: \nAuthor-email: \nLicense: new BSD\nLocation: /usr/local/lib/python3.11/dist-packages\nRequires: joblib, numpy, scipy, threadpoolctl\nRequired-by: bayesian-optimization, Boruta, category_encoders, cesium, eli5, fastai, hdbscan, hep_ml, imbalanced-learn, librosa, lime, mlxtend, nilearn, pyLDAvis, pynndescent, rgf-python, scikit-learn-intelex, scikit-optimize, scikit-plot, sentence-transformers, shap, sklearn-compat, sklearn-pandas, TPOT, tsfresh, umap-learn, woodwork, yellowbrick\nName: imbalanced-learn\nVersion: 0.10.1\nSummary: Toolbox for imbalanced dataset in machine learning.\nHome-page: https://github.com/scikit-learn-contrib/imbalanced-learn\nAuthor: \nAuthor-email: \nLicense: MIT\nLocation: /usr/local/lib/python3.11/dist-packages\nRequires: joblib, numpy, scikit-learn, scipy, threadpoolctl\nRequired-by: \nSMOTE imported successfully\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore', category=UserWarning)\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, precision_recall_curve, auc\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom category_encoders import TargetEncoder\nimport time\nimport shap\nimport matplotlib.pyplot as plt\nimport pyarrow.parquet as pq\n# from sklearn.ensemble import HistGradientBoostingClassifier  # Alternative model that handles NaNs\n\n# Verify SMOTEENN availability\ntry:\n    from imblearn.combine import SMOTEENN\n    print(\"SMOTEENN imported successfully\")\nexcept ImportError:\n    print(\"Error: SMOTEENN not available. Install: pip install imbalanced-learn==0.10.1\")\n    raise ImportError(\"SMOTEENN is required.\")\n\n# Data loading and preprocessing function\ndef prepare_data_for_targets(data_path, flag_map, batch_size=10000):\n    \"\"\"Load and preprocess data from Parquet file in batches.\"\"\"\n    processed_chunks = []\n    required_cols = ['MOTHER_ID', 'GRAVIDA']\n    numeric_cols = [\n        'GRAVIDA', 'PARITY', 'ABORTIONS', 'HEIGHT', 'HEMOGLOBIN_mean', 'HEMOGLOBIN_min', 'HEMOGLOBIN_max',\n        'WEIGHT_anc_mean', 'WEIGHT_anc_min', 'WEIGHT_anc_max', 'systolic_bp', 'diastolic_bp', 'BMI',\n        'weight_gain', 'weight_gain_per_week', 'TOTAL_ANC_VISITS', 'NO_OF_WEEKS_max', 'PHQ_SCORE_max', 'GAD_SCORE_max',\n        'gravida_parity_ratio'\n    ]\n    flag_cols = [\n        'age_adolescent', 'age_elderly', 'age_very_young', 'previous_loss', 'recurrent_loss', 'inadequate_anc',\n        'irregular_anc', 'anemia_mild', 'anemia_moderate', 'anemia_severe', 'ever_severe_anemia', 'hypertension',\n        'underweight', 'obese', 'normal_weight', 'inadequate_weight_gain'\n    ]\n    categorical_cols = ['FACILITY_TYPE', 'BLOOD_GRP', 'SYS_DISEASE']\n    \n    parquet_file = pq.ParquetFile(data_path)\n    num_rows = parquet_file.metadata.num_rows\n    num_batches = (num_rows + batch_size - 1) // batch_size\n    \n    for batch_idx in range(num_batches):\n        batch = parquet_file.read_row_group(batch_idx).to_pandas() if parquet_file.num_row_groups > batch_idx else pd.DataFrame()\n        if batch.empty:\n            continue\n        \n        for col in required_cols:\n            if col not in batch.columns:\n                raise ValueError(f\"Required column {col} missing in data\")\n        \n        # Convert numeric columns\n        for col in numeric_cols:\n            if col in batch.columns:\n                batch[col] = pd.to_numeric(batch[col], errors='coerce')\n        \n        # Map flag columns and handle NaNs\n        for col in flag_cols:\n            if col in batch.columns:\n                if batch[col].dtype == 'object':\n                    batch[col] = batch[col].map(flag_map)\n                batch[col] = batch[col].fillna(0)  # Impute NaNs with 0 for flags\n        \n        # Impute numeric NaNs with median\n        if numeric_cols:\n            batch[numeric_cols] = batch[numeric_cols].fillna(batch[numeric_cols].median())\n        \n        # Impute categorical NaNs with mode\n        for col in categorical_cols:\n            if col in batch.columns:\n                mode_val = batch[col].mode().iloc[0] if not batch[col].mode().empty else 'Unknown'\n                batch[col] = batch[col].fillna(mode_val)\n        \n        processed_chunks.append(batch)\n    \n    df = pd.concat(processed_chunks, ignore_index=True)\n    \n    # Limit SYS_DISEASE to top 10 categories\n    if 'SYS_DISEASE' in df.columns:\n        top_categories = df['SYS_DISEASE'].value_counts().index[:10]\n        df['SYS_DISEASE'] = df['SYS_DISEASE'].apply(lambda x: x if x in top_categories else 'Other')\n    \n    # Add interaction features and impute NaNs\n    # if 'anemia_mild' in df.columns and 'TOTAL_ANC_VISITS' in df.columns:\n    #     df['anemia_mild_anc_visits'] = df['anemia_mild'] * df['TOTAL_ANC_VISITS']\n    #     df['anemia_mild_anc_visits'] = df['anemia_mild_anc_visits'].fillna(df['anemia_mild_anc_visits'].median())\n    # if 'hypertension' in df.columns and 'WEIGHT_anc_mean' in df.columns:\n    #     df['hypertension_weight'] = df['hypertension'] * df['WEIGHT_anc_mean']\n    #     df['hypertension_weight'] = df['hypertension_weight'].fillna(df['hypertension_weight'].median())\n    if 'GRAVIDA' in df.columns and 'PARITY' in df.columns:\n        df['gravida_parity_ratio'] = df['GRAVIDA'] / (df['PARITY'] + 1e-5)  # Avoid division by zero\n        df['gravida_parity_ratio'] = df['gravida_parity_ratio'].fillna(df['gravida_parity_ratio'].median())\n    \n    # Final NaN check\n    nan_cols = df[numeric_cols + flag_cols + categorical_cols].isna().sum()\n    if nan_cols.any():\n        print(\"Warning: NaNs remain in the following columns after preprocessing:\")\n        print(nan_cols[nan_cols > 0])\n    \n    return df\n\n# Stratified sampling function\ndef create_stratified_sample(df, target_column, sample_size=10000, min_positive=1000):\n    \"\"\"Create a stratified sample ensuring both classes are included.\"\"\"\n    if target_column not in df.columns:\n        raise ValueError(f\"Target column '{target_column}' not found in DataFrame. Available columns: {df.columns.tolist()}\")\n    \n    positive_cases = df[df[target_column] == 1]\n    negative_cases = df[df[target_column] == 0]\n    \n    print(f\"Found {len(positive_cases)} positive cases and {len(negative_cases)} negative cases for {target_column}\")\n    \n    if len(positive_cases) == 0:\n        raise ValueError(f\"No positive cases ({target_column}=1) found.\")\n    if len(negative_cases) == 0:\n        raise ValueError(f\"No negative cases ({target_column}=0) found.\")\n    \n    sampled_negatives = negative_cases\n    target_negatives = len(negative_cases)\n    target_positives = min(len(positive_cases), max(min_positive, target_negatives * 2))  # Aim for 2:1 positive:negative\n    remaining_size = min(sample_size - len(sampled_negatives), len(positive_cases))\n    \n    if remaining_size > 0:\n        sampled_positives = positive_cases.sample(n=remaining_size, random_state=42)\n        final_sample = pd.concat([sampled_positives, sampled_negatives], ignore_index=True)\n    else:\n        final_sample = sampled_negatives\n    \n    print(f\"Sampled {len(sampled_positives)} positive cases and {len(sampled_negatives)} negative cases\")\n    return final_sample\n\n# Main script\ndata_path = '/kaggle/input/fullfinal/telangana_data_with_features_and_targets (1).parquet'\nflag_map = {\n    'Y': 1, 'YES': 1, 'Yes': 1, 'y': 1, 'yes': 1,\n    'N': 0, 'NO': 0, 'No': 0, 'n': 0, 'no': 0,\n    None: np.nan, 'None': np.nan, '': np.nan, 'nan': np.nan\n}\n\n# Load and preprocess data\ndf = prepare_data_for_targets(data_path, flag_map)\n\n# Define columns to exclude to prevent data leakage\nexclude_cols = [\n    'anc_dropout', 'maternal_mortality_risk', 'stillbirth_risk', 'premature_birth_risk', 'birth_defect_risk',\n    'high_risk_pregnancy', 'total_risk_factors', 'clinical_risk_score', 'risk_level', 'predicted_risk',\n    'total_missed_visits', 'age_risk_score', 'demographic_risk', 'anemia_risk_score', 'overall_risk_score',\n    'MOTHER_ID', 'ANC_ID', 'CHILD_ID', 'unique_id', 'EID', 'UID_NUMBER', 'WEIGHT_child_mean', 'WEIGHT_child_min',\n    'DELIVERY_MODE', 'MATERNAL_OUTCOME', 'IS_DELIVERED', 'DELIVERY_OUTCOME', 'DATE_OF_DELIVERY', 'PLACE_OF_DELIVERY',\n    'DEL_TIME', 'DATE_OF_DISCHARGE', 'DISCHARGE_TIME', 'JSY_BENEFICIARY', 'IS_MOTHER_ALIVE', 'IS_CHILD_DEATH',\n    'CHILD_DEATH_DATE', 'CHILD_DEATH_REASON', 'DEFECT_HEALTH_CENTER', 'IS_DEFECTIVE_BIRTH', 'BIRTH_DEFECT_TYPE',\n    'BIRTH_DEFECT_SUBTYPE', 'DEFECT_SUBTYPE_OTHER', 'DEFECT_TYPE_OTHER', 'NOTIFICATION_SENT',\n    'FBIR_COMPLETED_BY_ANM', 'NEWBORN_SCREENING', 'DEATH_REASON_OTHER', 'SNCU_ADMITTED', 'SNCU_REFERRAL_HOSPITAL',\n    'TERTIARY_REFERRAL_HOSPITAL', 'OTHER_REFERRAL_HOSPITAL', 'DATE_OF_DEATH', 'REASON_FOR_DEATH', 'PLACE_OF_DEATH',\n    'INDICATION_FOR_C_SECTION', 'DELIVERY_INSTITUTION', 'DELIVERY_DONE_BY', 'CONDUCT_BY', 'MISOPROSTAL_TABLET',\n    'DEL_COMPLICATIONS', 'OTHER_DEL_COMPLICATIONS', 'NOTIFICATION_SENT_del', 'FBIR_COMPLETED_BY_ANM_del',\n    'REGISTRATION_DT', 'REGTYPE', 'CURRENT_USR', 'OTHER_STATE_PLACE', 'OTHER_STATE_PLACE_FILEPATH',\n    'OTHER_GOVT_PLACE_FILEPATH', 'ANC2_TAG_FAC_ID', 'ANC3_TAG_FAC_ID', 'ANC_INSTITUTE', 'FACILITY_NAME',\n    'DOCTOR_ANM', 'DISTRICT_anc', 'DISTRICT_child', 'IS_BF_IN_HOUR', 'FEEDING_TYPE', 'DATE_OF_FIRST_FEEDING',\n    'TIME_OF_FIRST_FEEDING', 'BABY_ON_MEDICATION', 'MEDICATION_REMARKS', 'DATE_OF_BLOODSAMPLE_COLLECTION',\n    'TIME_OF_BLOODSAMPLE_COLLECTION', 'HOURS_OF_SAMPLE_COLLECTION', 'TRANSFUSION_DONE', 'VDRL_DATE',\n    'VDRL_STATUS', 'HIV_DATE', 'HIV_STATUS', 'HBSAG_DATE', 'HBSAG_STATUS', 'HEP_DATE', 'HEP_STATUS',\n    'VDRL_RESULT', 'HIV_RESULT', 'HBSAG_RESULT', 'HEP_RESULT', 'MISSANC1FLG', 'MISSANC2FLG', 'MISSANC3FLG',\n    'MISSANC4FLG', 'HIGH_RISKS', 'DISEASES', 'CHILD_NAME', 'age_category', 'multigravida', 'grand_multipara',\n    'no_anc', 'missed_first_anc', 'consecutive_missed', 'severe_hypertension', 'low_birth_weight',\n    'very_low_birth_weight', 'avg_birth_weight_low', 'depression', 'severe_depression', 'anxiety',\n    'severe_anxiety', 'hemoglobin_trend', 'TT_DATE', 'MAL_PRESENT', 'IS_ADMITTED_SNCU', 'IS_PREV_PREG',\n    'ANC1FLG', 'ANC2FLG', 'ANC3FLG', 'ANC4FLG', 'ANC_DATE', 'DEATH', 'EXP_DOD', 'DELIVERY_PLACE',\n    'EXP_DOD_preg', 'FASTING', 'LMP_DT', 'SCREENED_FOR_MENTAL_HEALTH', 'AGE', 'AGE_final', 'AGE_preg',\n    'GENDER', 'TIME_OF_BIRTH', 'TWIN_PREGNANCY_max', 'RNK', 'PHQ_SCORE_max', 'GAD_SCORE_max',\n    'mental_health_risk', 'NO_OF_WEEKS_max', 'DELIVERY_INSTITUTION', 'DELIVERY_DONE_BY', 'CONDUCT_BY',\n    'OTHER_NAME', 'NOTIFICATION_SENT', 'FBIR_COMPLETED_BY_ANM', 'OTHER_STATE_PLACE', 'OTHER_STATE_PLACE_FILEPATH',\n    'OTHER_GOVT_PLACE_FILEPATH', 'bp_risk'\n]\n\n# Diagnostic checks\nprint(\"Columns in df:\", df.columns.tolist())\nprint(\"anc_dropout distribution:\")\nprint(df['anc_dropout'].value_counts(dropna=False))\n\n# Select target\ntarget_column = 'anc_dropout'\nif df[target_column].eq(1).sum() == 0:\n    raise ValueError(f\"No positive cases found for {target_column}. Cannot proceed.\")\nprint(f\"Using target: {target_column}\")\n\n# Create sample\nsample_df = create_stratified_sample(df, target_column, sample_size=10000)\nprint(f\"Class distribution in sample_df for {target_column}:\")\nprint(sample_df[target_column].value_counts())\n\n# Select features\nnumeric_features = [\n    'GRAVIDA', 'PARITY', 'ABORTIONS', 'HEIGHT', 'HEMOGLOBIN_mean', 'HEMOGLOBIN_min', 'HEMOGLOBIN_max',\n    'WEIGHT_anc_mean', 'WEIGHT_anc_min', 'WEIGHT_anc_max', 'systolic_bp', 'diastolic_bp', 'BMI',\n    'weight_gain', 'weight_gain_per_week', 'gravida_parity_ratio', 'TOTAL_ANC_VISITS', 'NO_OF_WEEKS_max'\n    # 'hypertension_weight'\n]\nflag_features = [\n    'age_adolescent', 'age_elderly', 'age_very_young', 'previous_loss', 'recurrent_loss', 'inadequate_anc',\n    'irregular_anc', 'anemia_mild', 'anemia_moderate', 'anemia_severe', 'ever_severe_anemia', 'hypertension',\n    'underweight', 'obese', 'normal_weight', 'inadequate_weight_gain'\n]\ncategorical_features = ['FACILITY_TYPE', 'BLOOD_GRP', 'SYS_DISEASE']\nfeatures = numeric_features + flag_features + categorical_features\n\nif not features:\n    raise ValueError(f\"No valid features available for {target_column}. Available columns: {sample_df.columns.tolist()}\")\nprint(\"Features used:\", features)\n\n# Prepare data\nX = sample_df[features]\ny = sample_df[target_column]\n\n# Check for NaNs in features\nnan_summary = X.isna().sum()\nif nan_summary.any():\n    print(\"NaNs found in features before encoding:\")\n    print(nan_summary[nan_summary > 0])\n\n# Encode categorical features\nencoder = TargetEncoder(cols=categorical_features)\nX = encoder.fit_transform(X, y)\n\n# Check for NaNs after encoding\nnan_summary = X.isna().sum()\nif nan_summary.any():\n    print(\"NaNs found in features after encoding:\")\n    print(nan_summary[nan_summary > 0])\n\n# Initial train-test split\nX_train_full, X_test, y_train_full, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Initialize k-fold cross-validation\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Metrics storage\nmetrics = {\n    'thresh_0_1': {'f1': [], 'accuracy': [], 'precision': [], 'recall': [], 'pr_auc': []},\n    'thresh_0_2': {'f1': [], 'accuracy': [], 'precision': [], 'recall': [], 'pr_auc': []},\n    'thresh_0_3': {'f1': [], 'accuracy': [], 'precision': [], 'recall': [], 'pr_auc': []},\n    'thresh_0_4': {'f1': [], 'accuracy': [], 'precision': [], 'recall': [], 'pr_auc': []},\n    'auc': []\n}\nfold_models = []\nfold_times = []\nfold_avg_f1 = []\n\n# Calculate scale_pos_weight\nneg_count = (y_train_full == 0).sum()\npos_count = (y_train_full == 1).sum()\nscale_pos_weight = neg_count / pos_count * 1.5 if pos_count > 0 else 1.0\nprint(f\"Scale pos weight: {scale_pos_weight:.2f}\")\n\n# LightGBM parameters\nparams = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'n_estimators': 500,\n    'max_depth': 7,\n    'learning_rate': 0.05,\n    'scale_pos_weight': scale_pos_weight,\n    'min_child_weight': 5,\n    'random_state': 42,\n    'n_jobs': -1,\n    'verbosity': -1\n}\n\n# K-fold cross-validation\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X_train_full, y_train_full)):\n    print(f\"Training fold {fold + 1}/{n_splits}\")\n    \n    X_train, X_val = X_train_full.iloc[train_idx], X_train_full.iloc[val_idx]\n    y_train, y_val = y_train_full.iloc[train_idx], y_train_full.iloc[val_idx]\n    \n    print(f\"Training class counts:\\n{y_train.value_counts()}\")\n    print(f\"Validation class counts:\\n{y_val.value_counts()}\")\n    \n    if len(y_train.unique()) < 2 or len(y_val.unique()) < 2:\n        print(f\"Warning: Fold {fold + 1} has only one class. Skipping.\")\n        continue\n    \n    # Apply SMOTEENN with imputation pipeline\n    print(f\"Applying SMOTEENN for fold {fold + 1}\")\n    imputer = SimpleImputer(strategy='median')\n    smoteenn = SMOTEENN(random_state=42, sampling_strategy=0.5)\n    pipeline = Pipeline([\n        ('imputer', imputer),\n        ('smoteenn', smoteenn)\n    ])\n    \n    try:\n        X_train, y_train = pipeline.fit_resample(X_train, y_train)\n        print(f\"Post-SMOTEENN training class counts:\\n{y_train.value_counts()}\")\n    except ValueError as e:\n        print(f\"SMOTEENN pipeline failed for fold {fold + 1}: {e}. Proceeding without resampling.\")\n    \n    start_time = time.time()\n    model = lgb.LGBMClassifier(**params)\n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='auc', callbacks=[lgb.early_stopping(50, verbose=False)])\n    fold_time = time.time() - start_time\n    fold_models.append(model)\n    fold_times.append(fold_time)\n    \n    y_pred_proba = model.predict_proba(X_val)[:, 1]\n    precisions, recalls, pr_thresholds = precision_recall_curve(y_val, y_pred_proba)\n    pr_auc = auc(recalls, precisions)\n    \n    for thresh, thresh_name in [(0.1, 'thresh_0_1'), (0.2, 'thresh_0_2'), (0.3, 'thresh_0_3'), (0.4, 'thresh_0_4')]:\n        y_pred = (y_pred_proba > thresh).astype(int)\n        metrics[thresh_name]['f1'].append(f1_score(y_val, y_pred))\n        metrics[thresh_name]['accuracy'].append(accuracy_score(y_val, y_pred))\n        metrics[thresh_name]['precision'].append(precision_score(y_val, y_pred, zero_division=0))\n        metrics[thresh_name]['recall'].append(recall_score(y_val, y_pred, zero_division=0))\n        metrics[thresh_name]['pr_auc'].append(pr_auc)\n    \n    auc_score = roc_auc_score(y_val, y_pred_proba) if len(np.unique(y_val)) > 1 else 0\n    metrics['auc'].append(auc_score)\n    \n    avg_f1 = np.mean([metrics[f'thresh_0_{i}']['f1'][-1] for i in [1, 2, 3, 4]])\n    fold_avg_f1.append(avg_f1)\n    \n    print(f\"Fold {fold + 1}\")\n    print(f\"  AUC: {auc_score:.4f}, PR-AUC: {pr_auc:.4f}, Time: {fold_time:.2f} seconds\")\n    for thresh, thresh_name in [(0.1, 'thresh_0_1'), (0.2, 'thresh_0_2'), (0.3, 'thresh_0_3'), (0.4, 'thresh_0_4')]:\n        print(f\"  Threshold {thresh} - F1: {metrics[thresh_name]['f1'][-1]:.4f}, Accuracy: {metrics[thresh_name]['accuracy'][-1]:.4f}, \"\n              f\"Precision: {metrics[thresh_name]['precision'][-1]:.4f}, Recall: {metrics[thresh_name]['recall'][-1]:.4f}\")\n        print(f\"  Confusion Matrix (Threshold {thresh}):\\n{confusion_matrix(y_val, (y_pred_proba > thresh).astype(int))}\")\n\n# Cross-validation results\nprint(f\"\\nCross-Validation Mean Metrics:\")\nprint(f\"  AUC: {np.mean(metrics['auc']):.4f} ± {np.std(metrics['auc']):.4f}\")\nfor thresh_name in ['thresh_0_1', 'thresh_0_2', 'thresh_0_3', 'thresh_0_4']:\n    print(f\"\\n{thresh_name.replace('_', ' ').title()}:\")\n    print(f\"  F1 Score: {np.mean(metrics[thresh_name]['f1']):.4f} ± {np.std(metrics[thresh_name]['f1']):.4f}\")\n    print(f\"  Accuracy: {np.mean(metrics[thresh_name]['accuracy']):.4f} ± {np.std(metrics[thresh_name]['accuracy']):.4f}\")\n    print(f\"  Precision: {np.mean(metrics[thresh_name]['precision']):.4f} ± {np.std(metrics[thresh_name]['precision']):.4f}\")\n    print(f\"  Recall: {np.mean(metrics[thresh_name]['recall']):.4f} ± {np.std(metrics[thresh_name]['recall']):.4f}\")\n    print(f\"  PR-AUC: {np.mean(metrics[thresh_name]['pr_auc']):.4f} ± {np.std(metrics[thresh_name]['pr_auc']):.4f}\")\n\n# Evaluate best model on test set\nif fold_avg_f1:\n    best_fold_idx = np.argmax(fold_avg_f1)\n    best_model = fold_models[best_fold_idx]\n    print(f\"\\nBest Model from Fold {best_fold_idx + 1} with Average F1 Score: {fold_avg_f1[best_fold_idx]:.4f}\")\n    \n    y_test_pred_proba = best_model.predict_proba(X_test)[:, 1]\n    precisions, recalls, _ = precision_recall_curve(y_test, y_test_pred_proba)\n    test_pr_auc = auc(recalls, precisions)\n    \n    test_metrics = {}\n    for thresh in [0.1, 0.2, 0.3, 0.4]:\n        y_test_pred = (y_test_pred_proba > thresh).astype(int)\n        test_metrics[thresh] = {\n            'f1': f1_score(y_test, y_test_pred),\n            'accuracy': accuracy_score(y_test, y_test_pred),\n            'precision': precision_score(y_test, y_test_pred, zero_division=0),\n            'recall': recall_score(y_test, y_test_pred, zero_division=0),\n            'cm': confusion_matrix(y_test, y_test_pred)\n        }\n    \n    test_auc = roc_auc_score(y_test, y_test_pred_proba) if len(np.unique(y_test)) > 1 else 0\n    print(f\"\\nTest Set Metrics (Best Model from Fold {best_fold_idx + 1}):\")\n    print(f\"  AUC: {test_auc:.4f}, PR-AUC: {test_pr_auc:.4f}\")\n    for thresh in [0.1, 0.2, 0.3, 0.4]:\n        print(f\"\\nThreshold {thresh}:\\n  F1: {test_metrics[thresh]['f1']:.4f}, Accuracy: {test_metrics[thresh]['accuracy']:.4f}, \"\n              f\"Precision: {test_metrics[thresh]['precision']:.4f}, Recall: {test_metrics[thresh]['recall']:.4f}\\n  \"\n              f\"Confusion Matrix:\\n{test_metrics[thresh]['cm']}\")\n    \n    test_f1_scores = {thresh: test_metrics[thresh]['f1'] for thresh in [0.1, 0.2, 0.3, 0.4]}\n    best_threshold = max(test_f1_scores, key=test_f1_scores.get)\n    print(f\"\\nBest Threshold on Test Set: {best_threshold} with F1 Score: {test_f1_scores[best_threshold]:.4f}\")\n    \n    # SHAP analysis\n    print(\"\\nPerforming SHAP analysis...\")\n    X_test_sample = X_test.sample(n=min(1000, len(X_test)), random_state=42)\n    explainer = shap.TreeExplainer(best_model)\n    shap_values = explainer.shap_values(X_test_sample)[1]\n    \n    plt.figure()\n    shap.summary_plot(shap_values, X_test_sample, show=False)\n    plt.savefig(\"shap_summary_plot_anc_dropout.png\")\n    plt.close()\n    print(\"SHAP summary plot saved as 'shap_summary_plot_anc_dropout.png'\")\n    \n    plt.figure()\n    shap.summary_plot(shap_values, X_test_sample, plot_type=\"bar\", show=False)\n    plt.savefig(\"shap_importance_bar_anc_dropout.png\")\n    plt.close()\n    print(\"SHAP feature importance bar plot saved as 'shap_importance_bar_anc_dropout.png'\")\n    \n    shap_importance = np.abs(shap_values).mean(axis=0)\n    importance_df = pd.DataFrame({\n        'Feature': X_test_sample.columns,\n        'SHAP_Importance': shap_importance\n    }).sort_values(by='SHAP_Importance', ascending=False)\n    print(\"\\nSHAP Feature Importance:\")\n    print(importance_df)\nelse:\n    print(\"\\nNo valid models trained due to single-class folds.\")\n\n# Alternative model (uncomment to use)\n\"\"\"\n# Use HistGradientBoostingClassifier which handles NaNs natively\nparams = {\n    'max_iter': 500,\n    'max_depth': 7,\n    'learning_rate': 0.05,\n    'random_state': 42\n}\nmodel = HistGradientBoostingClassifier(**params)\n\"\"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T09:58:28.586747Z","iopub.execute_input":"2025-06-17T09:58:28.588212Z","iopub.status.idle":"2025-06-17T09:59:43.812006Z","shell.execute_reply.started":"2025-06-17T09:58:28.588169Z","shell.execute_reply":"2025-06-17T09:59:43.810938Z"}},"outputs":[{"name":"stdout","text":"SMOTEENN imported successfully\nColumns in df: ['ANC_ID', 'MOTHER_ID', 'GRAVIDA', 'ANC_DATE', 'ANC_INSTITUTE', 'FACILITY_TYPE', 'FACILITY_NAME', 'DOCTOR_ANM', 'BLOOD_SUGAR', 'IFA_TABLET', 'TT_GIVEN', 'USG_SCAN', 'MAL_PRESENT', 'PLACENTA', 'HIV', 'THYROID', 'RH_NEGATIVE', 'SYS_DISEASE', 'DISTRICT_anc', 'EXP_DOD', 'ANC_TYPE', 'URINE_TEST', 'URINE_SUGAR', 'URINE_ALBUMIN', 'UTERUS_SIZE', 'HEART_RATE', 'FOETAL_POSITION', 'FASTING', 'TT_DATE', 'POST_PRANDIAL', 'BLOOD_SUGAR_TEST', 'IRON_SUCROSE_INJ', 'SCREENED_FOR_MENTAL_HEALTH', 'PARA_TO_WARNING_SIGNS_HTN', 'RNK', 'unique_id', 'CHILD_ID', 'GENDER', 'TIME_OF_BIRTH', 'IS_DEFECTIVE_BIRTH', 'IS_BF_IN_HOUR', 'AGE', 'DELIVERY_PLACE', 'DELIVERY_MODE', 'IS_PREV_PREG', 'IS_ADMITTED_SNCU', 'SNCU_REFERRAL_HOSPITAL', 'TERTIARY_REFERRAL_HOSPITAL', 'OTHER_REFERRAL_HOSPITAL', 'CHILD_DEATH_DATE', 'CHILD_DEATH_REASON', 'IMMUNE_CYCLE_DONE', 'DISTRICT_child', 'DEFECT_HEALTH_CENTER', 'IS_CHILD_DEATH', 'BIRTH_DEFECT_TYPE', 'BIRTH_DEFECT_SUBTYPE', 'DEFECT_SUBTYPE_OTHER', 'NOTIFICATION_SENT', 'FBIR_COMPLETED_BY_ANM', 'CHILD_NAME', 'BIRTH_SCREENING', 'DEFECT_TYPE_OTHER', 'DEATH_REASON_OTHER', 'NEWBORN_SCREENING', 'EID', 'UID_NUMBER', 'SNCU_ADMITTED', 'CONSANGUINITY', 'HIGH_RISKS', 'DISEASES', 'FEEDING_TYPE', 'DATE_OF_FIRST_FEEDING', 'TIME_OF_FIRST_FEEDING', 'DATE_OF_BLOODSAMPLE_COLLECTION', 'TIME_OF_BLOODSAMPLE_COLLECTION', 'HOURS_OF_SAMPLE_COLLECTION', 'TRANSFUSION_DONE', 'BABY_ON_MEDICATION', 'CH', 'CAH', 'GALACTOCEMIA', 'G6PDD', 'BIOTINIDASE', 'MEDICATION_REMARKS', 'DATE_OF_DELIVERY', 'PLACE_OF_DELIVERY', 'DELIVERY_OUTCOME', 'MODE_OF_DELIVERY', 'MATERNAL_OUTCOME', 'REASON_FOR_DEATH', 'DATE_OF_DEATH', 'PLACE_OF_DEATH', 'INDICATION_FOR_C_SECTION', 'DELIVERY_INSTITUTION', 'DELIVERY_DONE_BY', 'IS_DELIVERED', 'DATE_OF_DISCHARGE', 'IS_MOTHER_ALIVE', 'DEL_TIME', 'MISOPROSTAL_TABLET', 'CONDUCT_BY', 'OTHER_NAME', 'JSY_BENEFICIARY', 'DISCHARGE_TIME', 'DEL_COMPLICATIONS', 'OTHER_DEL_COMPLICATIONS', 'NOTIFICATION_SENT_del', 'FBIR_COMPLETED_BY_ANM_del', 'OTHER_STATE_PLACE', 'OTHER_STATE_PLACE_FILEPATH', 'OTHER_GOVT_PLACE_FILEPATH', 'REGISTRATION_DT', 'LMP_DT', 'EXP_DOD_preg', 'PARITY', 'ABORTIONS', 'LIVE', 'DEATH', 'OBSTETRIC_FORMULA', 'AGE_preg', 'HEIGHT', 'BLOOD_GRP', 'ANC1FLG', 'ANC2FLG', 'ANC3FLG', 'ANC4FLG', 'MISSANC1FLG', 'MISSANC2FLG', 'MISSANC3FLG', 'MISSANC4FLG', 'REGTYPE', 'CURRENT_USR', 'ANC2_TAG_FAC_DIST', 'ANC2_TAG_FAC_ID', 'ANC3_TAG_FAC_DIST', 'ANC3_TAG_FAC_ID', 'VDRL_DATE', 'VDRL_STATUS', 'VDRL_RESULT', 'HIV_DATE', 'HIV_STATUS', 'HIV_RESULT', 'HBSAG_DATE', 'HBSAG_STATUS', 'HBSAG_RESULT', 'HEP_DATE', 'HEP_STATUS', 'HEP_RESULT', 'HEMOGLOBIN_mean', 'HEMOGLOBIN_min', 'HEMOGLOBIN_max', 'WEIGHT_anc_mean', 'WEIGHT_anc_min', 'WEIGHT_anc_max', 'BP_last', 'NO_OF_WEEKS_max', 'TWIN_PREGNANCY_max', 'PHQ_SCORE_max', 'GAD_SCORE_max', 'TOTAL_ANC_VISITS', 'WEIGHT_child_mean', 'WEIGHT_child_min', 'AGE_final', 'age_adolescent', 'age_elderly', 'age_very_young', 'age_risk_score', 'age_category', 'multigravida', 'grand_multipara', 'previous_loss', 'recurrent_loss', 'gravida_parity_ratio', 'inadequate_anc', 'no_anc', 'total_missed_visits', 'irregular_anc', 'missed_first_anc', 'consecutive_missed', 'anemia_mild', 'anemia_moderate', 'anemia_severe', 'ever_severe_anemia', 'anemia_risk_score', 'hemoglobin_trend', 'systolic_bp', 'diastolic_bp', 'hypertension', 'severe_hypertension', 'bp_risk', 'BMI', 'underweight', 'obese', 'normal_weight', 'depression', 'severe_depression', 'anxiety', 'severe_anxiety', 'mental_health_risk', 'weight_gain', 'weight_gain_per_week', 'inadequate_weight_gain', 'low_birth_weight', 'very_low_birth_weight', 'avg_birth_weight_low', 'demographic_risk', 'clinical_risk_score', 'overall_risk_score', 'maternal_mortality_risk', 'stillbirth_risk', 'premature_birth_risk', 'anc_dropout', 'total_risk_factors', 'high_risk_pregnancy']\nanc_dropout distribution:\nanc_dropout\n1    4029441\n0        130\nName: count, dtype: int64\nUsing target: anc_dropout\nFound 4029441 positive cases and 130 negative cases for anc_dropout\nSampled 9870 positive cases and 130 negative cases\nClass distribution in sample_df for anc_dropout:\nanc_dropout\n1    9870\n0     130\nName: count, dtype: int64\nFeatures used: ['GRAVIDA', 'PARITY', 'ABORTIONS', 'HEIGHT', 'HEMOGLOBIN_mean', 'HEMOGLOBIN_min', 'HEMOGLOBIN_max', 'WEIGHT_anc_mean', 'WEIGHT_anc_min', 'WEIGHT_anc_max', 'systolic_bp', 'diastolic_bp', 'BMI', 'weight_gain', 'weight_gain_per_week', 'gravida_parity_ratio', 'TOTAL_ANC_VISITS', 'NO_OF_WEEKS_max', 'age_adolescent', 'age_elderly', 'age_very_young', 'previous_loss', 'recurrent_loss', 'inadequate_anc', 'irregular_anc', 'anemia_mild', 'anemia_moderate', 'anemia_severe', 'ever_severe_anemia', 'hypertension', 'underweight', 'obese', 'normal_weight', 'inadequate_weight_gain', 'FACILITY_TYPE', 'BLOOD_GRP', 'SYS_DISEASE']\nScale pos weight: 0.02\nTraining fold 1/5\nTraining class counts:\nanc_dropout\n1    6316\n0      84\nName: count, dtype: int64\nValidation class counts:\nanc_dropout\n1    1580\n0      20\nName: count, dtype: int64\nApplying SMOTEENN for fold 1\nPost-SMOTEENN training class counts:\nanc_dropout\n1    6316\n0    3158\nName: count, dtype: int64\nFold 1\n  AUC: 1.0000, PR-AUC: 1.0000, Time: 0.10 seconds\n  Threshold 0.1 - F1: 0.9937, Accuracy: 0.9875, Precision: 0.9875, Recall: 1.0000\n  Confusion Matrix (Threshold 0.1):\n[[   0   20]\n [   0 1580]]\n  Threshold 0.2 - F1: 0.9937, Accuracy: 0.9875, Precision: 0.9875, Recall: 1.0000\n  Confusion Matrix (Threshold 0.2):\n[[   0   20]\n [   0 1580]]\n  Threshold 0.3 - F1: 0.9937, Accuracy: 0.9875, Precision: 0.9875, Recall: 1.0000\n  Confusion Matrix (Threshold 0.3):\n[[   0   20]\n [   0 1580]]\n  Threshold 0.4 - F1: 0.9937, Accuracy: 0.9875, Precision: 0.9875, Recall: 1.0000\n  Confusion Matrix (Threshold 0.4):\n[[   0   20]\n [   0 1580]]\nTraining fold 2/5\nTraining class counts:\nanc_dropout\n1    6317\n0      83\nName: count, dtype: int64\nValidation class counts:\nanc_dropout\n1    1579\n0      21\nName: count, dtype: int64\nApplying SMOTEENN for fold 2\nPost-SMOTEENN training class counts:\nanc_dropout\n1    6317\n0    3158\nName: count, dtype: int64\nFold 2\n  AUC: 1.0000, PR-AUC: 1.0000, Time: 0.06 seconds\n  Threshold 0.1 - F1: 0.9934, Accuracy: 0.9869, Precision: 0.9869, Recall: 1.0000\n  Confusion Matrix (Threshold 0.1):\n[[   0   21]\n [   0 1579]]\n  Threshold 0.2 - F1: 0.9934, Accuracy: 0.9869, Precision: 0.9869, Recall: 1.0000\n  Confusion Matrix (Threshold 0.2):\n[[   0   21]\n [   0 1579]]\n  Threshold 0.3 - F1: 0.9934, Accuracy: 0.9869, Precision: 0.9869, Recall: 1.0000\n  Confusion Matrix (Threshold 0.3):\n[[   0   21]\n [   0 1579]]\n  Threshold 0.4 - F1: 0.9934, Accuracy: 0.9869, Precision: 0.9869, Recall: 1.0000\n  Confusion Matrix (Threshold 0.4):\n[[   0   21]\n [   0 1579]]\nTraining fold 3/5\nTraining class counts:\nanc_dropout\n1    6317\n0      83\nName: count, dtype: int64\nValidation class counts:\nanc_dropout\n1    1579\n0      21\nName: count, dtype: int64\nApplying SMOTEENN for fold 3\nPost-SMOTEENN training class counts:\nanc_dropout\n1    6317\n0    3158\nName: count, dtype: int64\nFold 3\n  AUC: 1.0000, PR-AUC: 1.0000, Time: 0.06 seconds\n  Threshold 0.1 - F1: 0.9934, Accuracy: 0.9869, Precision: 0.9869, Recall: 1.0000\n  Confusion Matrix (Threshold 0.1):\n[[   0   21]\n [   0 1579]]\n  Threshold 0.2 - F1: 0.9934, Accuracy: 0.9869, Precision: 0.9869, Recall: 1.0000\n  Confusion Matrix (Threshold 0.2):\n[[   0   21]\n [   0 1579]]\n  Threshold 0.3 - F1: 0.9934, Accuracy: 0.9869, Precision: 0.9869, Recall: 1.0000\n  Confusion Matrix (Threshold 0.3):\n[[   0   21]\n [   0 1579]]\n  Threshold 0.4 - F1: 0.9934, Accuracy: 0.9869, Precision: 0.9869, Recall: 1.0000\n  Confusion Matrix (Threshold 0.4):\n[[   0   21]\n [   0 1579]]\nTraining fold 4/5\nTraining class counts:\nanc_dropout\n1    6317\n0      83\nName: count, dtype: int64\nValidation class counts:\nanc_dropout\n1    1579\n0      21\nName: count, dtype: int64\nApplying SMOTEENN for fold 4\nPost-SMOTEENN training class counts:\nanc_dropout\n1    6317\n0    3158\nName: count, dtype: int64\nFold 4\n  AUC: 1.0000, PR-AUC: 1.0000, Time: 0.06 seconds\n  Threshold 0.1 - F1: 0.9934, Accuracy: 0.9869, Precision: 0.9869, Recall: 1.0000\n  Confusion Matrix (Threshold 0.1):\n[[   0   21]\n [   0 1579]]\n  Threshold 0.2 - F1: 0.9934, Accuracy: 0.9869, Precision: 0.9869, Recall: 1.0000\n  Confusion Matrix (Threshold 0.2):\n[[   0   21]\n [   0 1579]]\n  Threshold 0.3 - F1: 0.9934, Accuracy: 0.9869, Precision: 0.9869, Recall: 1.0000\n  Confusion Matrix (Threshold 0.3):\n[[   0   21]\n [   0 1579]]\n  Threshold 0.4 - F1: 0.9934, Accuracy: 0.9869, Precision: 0.9869, Recall: 1.0000\n  Confusion Matrix (Threshold 0.4):\n[[   0   21]\n [   0 1579]]\nTraining fold 5/5\nTraining class counts:\nanc_dropout\n1    6317\n0      83\nName: count, dtype: int64\nValidation class counts:\nanc_dropout\n1    1579\n0      21\nName: count, dtype: int64\nApplying SMOTEENN for fold 5\nPost-SMOTEENN training class counts:\nanc_dropout\n1    6317\n0    3158\nName: count, dtype: int64\nFold 5\n  AUC: 1.0000, PR-AUC: 1.0000, Time: 0.06 seconds\n  Threshold 0.1 - F1: 0.9934, Accuracy: 0.9869, Precision: 0.9869, Recall: 1.0000\n  Confusion Matrix (Threshold 0.1):\n[[   0   21]\n [   0 1579]]\n  Threshold 0.2 - F1: 0.9934, Accuracy: 0.9869, Precision: 0.9869, Recall: 1.0000\n  Confusion Matrix (Threshold 0.2):\n[[   0   21]\n [   0 1579]]\n  Threshold 0.3 - F1: 0.9934, Accuracy: 0.9869, Precision: 0.9869, Recall: 1.0000\n  Confusion Matrix (Threshold 0.3):\n[[   0   21]\n [   0 1579]]\n  Threshold 0.4 - F1: 0.9934, Accuracy: 0.9869, Precision: 0.9869, Recall: 1.0000\n  Confusion Matrix (Threshold 0.4):\n[[   0   21]\n [   0 1579]]\n\nCross-Validation Mean Metrics:\n  AUC: 1.0000 ± 0.0000\n\nThresh 0 1:\n  F1 Score: 0.9935 ± 0.0001\n  Accuracy: 0.9870 ± 0.0003\n  Precision: 0.9870 ± 0.0003\n  Recall: 1.0000 ± 0.0000\n  PR-AUC: 1.0000 ± 0.0000\n\nThresh 0 2:\n  F1 Score: 0.9935 ± 0.0001\n  Accuracy: 0.9870 ± 0.0003\n  Precision: 0.9870 ± 0.0003\n  Recall: 1.0000 ± 0.0000\n  PR-AUC: 1.0000 ± 0.0000\n\nThresh 0 3:\n  F1 Score: 0.9935 ± 0.0001\n  Accuracy: 0.9870 ± 0.0003\n  Precision: 0.9870 ± 0.0003\n  Recall: 1.0000 ± 0.0000\n  PR-AUC: 1.0000 ± 0.0000\n\nThresh 0 4:\n  F1 Score: 0.9935 ± 0.0001\n  Accuracy: 0.9870 ± 0.0003\n  Precision: 0.9870 ± 0.0003\n  Recall: 1.0000 ± 0.0000\n  PR-AUC: 1.0000 ± 0.0000\n\nBest Model from Fold 1 with Average F1 Score: 0.9937\n\nTest Set Metrics (Best Model from Fold 1):\n  AUC: 1.0000, PR-AUC: 1.0000\n\nThreshold 0.1:\n  F1: 0.9935, Accuracy: 0.9870, Precision: 0.9870, Recall: 1.0000\n  Confusion Matrix:\n[[   0   26]\n [   0 1974]]\n\nThreshold 0.2:\n  F1: 0.9935, Accuracy: 0.9870, Precision: 0.9870, Recall: 1.0000\n  Confusion Matrix:\n[[   0   26]\n [   0 1974]]\n\nThreshold 0.3:\n  F1: 0.9935, Accuracy: 0.9870, Precision: 0.9870, Recall: 1.0000\n  Confusion Matrix:\n[[   0   26]\n [   0 1974]]\n\nThreshold 0.4:\n  F1: 0.9935, Accuracy: 0.9870, Precision: 0.9870, Recall: 1.0000\n  Confusion Matrix:\n[[   0   26]\n [   0 1974]]\n\nBest Threshold on Test Set: 0.1 with F1 Score: 0.9935\n\nPerforming SHAP analysis...\nSHAP summary plot saved as 'shap_summary_plot_anc_dropout.png'\nSHAP feature importance bar plot saved as 'shap_importance_bar_anc_dropout.png'\n\nSHAP Feature Importance:\n                   Feature  SHAP_Importance\n23          inadequate_anc     7.597500e-02\n10             systolic_bp     6.668277e-17\n12                     BMI     1.852685e-17\n34           FACILITY_TYPE     2.253297e-18\n0                  GRAVIDA     0.000000e+00\n22          recurrent_loss     0.000000e+00\n24           irregular_anc     0.000000e+00\n25             anemia_mild     0.000000e+00\n26         anemia_moderate     0.000000e+00\n27           anemia_severe     0.000000e+00\n28      ever_severe_anemia     0.000000e+00\n20          age_very_young     0.000000e+00\n29            hypertension     0.000000e+00\n30             underweight     0.000000e+00\n31                   obese     0.000000e+00\n32           normal_weight     0.000000e+00\n33  inadequate_weight_gain     0.000000e+00\n35               BLOOD_GRP     0.000000e+00\n21           previous_loss     0.000000e+00\n18          age_adolescent     0.000000e+00\n19             age_elderly     0.000000e+00\n8           WEIGHT_anc_min     0.000000e+00\n2                ABORTIONS     0.000000e+00\n3                   HEIGHT     0.000000e+00\n4          HEMOGLOBIN_mean     0.000000e+00\n5           HEMOGLOBIN_min     0.000000e+00\n6           HEMOGLOBIN_max     0.000000e+00\n7          WEIGHT_anc_mean     0.000000e+00\n9           WEIGHT_anc_max     0.000000e+00\n1                   PARITY     0.000000e+00\n11            diastolic_bp     0.000000e+00\n13             weight_gain     0.000000e+00\n14    weight_gain_per_week     0.000000e+00\n15    gravida_parity_ratio     0.000000e+00\n16        TOTAL_ANC_VISITS     0.000000e+00\n17         NO_OF_WEEKS_max     0.000000e+00\n36             SYS_DISEASE     0.000000e+00\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"\"\\n# Use HistGradientBoostingClassifier which handles NaNs natively\\nparams = {\\n    'max_iter': 500,\\n    'max_depth': 7,\\n    'learning_rate': 0.05,\\n    'random_state': 42\\n}\\nmodel = HistGradientBoostingClassifier(**params)\\n\""},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":" ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}