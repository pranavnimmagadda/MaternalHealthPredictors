{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12069575,"sourceType":"datasetVersion","datasetId":7597211}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score\nimport time\nimport warnings\nimport pyarrow.parquet as pq\nwarnings.filterwarnings('ignore')\n\ndata_path = '/kaggle/input/fullfinal/telangana_data_with_features_and_targets (1).parquet'\nflag_map = {\n    'Y': 1, 'YES': 1, 'Yes': 1, 'y': 1, 'yes': 1,\n    'N': 0, 'NO': 0, 'No': 0, 'n': 0, 'no': 0,\n    None: np.nan, 'None': np.nan, '': np.nan, 'nan': np.nan\n}\n\ndef prepare_data_for_targets(data_path, flag_map, batch_size=10000):\n    \"\"\"Load and preprocess data from Parquet file in batches.\"\"\"\n    # Initialize an empty list to store processed chunks\n    processed_chunks = []\n    \n    # Define required and numeric columns\n    required_cols = ['MOTHER_ID', 'GRAVIDA']\n    numeric_cols = ['AGE', 'AGE_preg', 'AGE_final', 'GRAVIDA', 'PARITY', 'ABORTIONS', 'TOTAL_ANC_VISITS',\n                    'HEMOGLOBIN_mean', 'HEMOGLOBIN_min', 'WEIGHT_max', 'HEIGHT',\n                    'PHQ_SCORE_max', 'GAD_SCORE_max', 'WEIGHT_last', 'WEIGHT_first',\n                    'NO_OF_WEEKS_max', 'WEIGHT_min', 'WEIGHT_mean']\n    \n    # Open Parquet file using pyarrow\n    parquet_file = pq.ParquetFile(data_path)\n    num_rows = parquet_file.metadata.num_rows\n    num_batches = (num_rows + batch_size - 1) // batch_size  # Ceiling division\n    \n    # Iterate through batches\n    for batch_idx in range(num_batches):\n        # Read a batch of rows\n        start_row = batch_idx * batch_size\n        end_row = min(start_row + batch_size, num_rows)\n        batch = parquet_file.read_row_group(batch_idx).to_pandas() if parquet_file.num_row_groups > batch_idx else pd.DataFrame()\n        \n        if batch.empty:\n            continue\n            \n        # Check for required columns\n        for col in required_cols:\n            if col not in batch.columns:\n                raise ValueError(f\"Required column {col} missing in data\")\n\n        # Debug: Check non-numeric values\n        for col in numeric_cols:\n            if col in batch.columns and batch[col].dtype == 'object':\n                non_numeric = batch[col][~batch[col].apply(lambda x: str(x).replace('.', '').isdigit() if pd.notna(x) else False)]\n                if not non_numeric.empty:\n                    print(f\"Non-numeric values in {col}: {non_numeric.unique()[:10]}\")\n\n        # Clean GRAVIDA specifically\n        if 'GRAVIDA' in batch.columns:\n            batch['GRAVIDA'] = batch['GRAVIDA'].replace('nan', np.nan)\n            batch['GRAVIDA'] = pd.to_numeric(batch['GRAVIDA'], errors='coerce')\n            # Replace NaN with median GRAVIDA (or 1 as default) within batch\n            if batch['GRAVIDA'].notna().any():\n                median_gravida = batch['GRAVIDA'].median()\n                if pd.isna(median_gravida):\n                    median_gravida = 1.0\n                batch['GRAVIDA'] = batch['GRAVIDA'].fillna(0)\n                print(f\"Filled {batch['GRAVIDA'].isna().sum()} GRAVIDA NaN values with 0 in batch\")\n\n        # Convert other numeric columns\n        for col in numeric_cols:\n            if col in batch.columns and col != 'GRAVIDA':\n                batch[col] = pd.to_numeric(batch[col], errors='coerce')\n\n        # Map flag columns\n        if 'IS_CHILD_DEATH' in batch.columns and batch['IS_CHILD_DEATH'].dtype == 'object':\n            batch['IS_CHILD_DEATH'] = batch['IS_CHILD_DEATH'].map(flag_map)\n        if 'IS_DEFECTIVE_BIRTH' in batch.columns and batch['IS_DEFECTIVE_BIRTH'].dtype == 'object':\n            batch['IS_DEFECTIVE_BIRTH'] = batch['IS_DEFECTIVE_BIRTH'].map(flag_map)\n\n        # Fill NaN for numeric columns\n        batch_numeric_cols = batch.select_dtypes(include=['float64', 'float32', 'int64', 'int32', 'int8']).columns\n        batch[batch_numeric_cols] = batch[batch_numeric_cols].fillna(0)\n\n        # Append processed batch\n        processed_chunks.append(batch)\n\n    # Concatenate all chunks\n    df = pd.concat(processed_chunks, ignore_index=True)\n    return df\n\n# Execute the function\ndf = prepare_data_for_targets(data_path, flag_map)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T11:44:53.766639Z","iopub.execute_input":"2025-06-17T11:44:53.767045Z","iopub.status.idle":"2025-06-17T11:45:44.051896Z","shell.execute_reply.started":"2025-06-17T11:44:53.767018Z","shell.execute_reply":"2025-06-17T11:45:44.050843Z"}},"outputs":[{"name":"stdout","text":"Non-numeric values in GRAVIDA: ['nan']\nFilled 0 GRAVIDA NaN values with 0 in batch\nFilled 0 GRAVIDA NaN values with 0 in batch\nFilled 0 GRAVIDA NaN values with 0 in batch\nFilled 0 GRAVIDA NaN values with 0 in batch\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"target_columns = [\n            'stillbirth_risk'\n        ]\ndef create_stratified_sample(df, target_column, sample_size=2000000):\n    \"\"\"Create a stratified sample ensuring all critical cases are included.\"\"\"\n    child_death_col = 'IS_CHILD_DEATH'\n    critical_cases = pd.DataFrame()\n    \n    if child_death_col in df.columns:\n        child_deaths = df[df[child_death_col] == 1]\n        critical_cases = pd.concat([critical_cases, child_deaths])\n    if 'maternal_mortality_risk' in df.columns:\n        maternal_deaths = df[df['maternal_mortality_risk'] == 1]\n        critical_cases = pd.concat([critical_cases, maternal_deaths])\n    if 'stillbirth_risk' in df.columns:\n        stillbirths = df[df['stillbirth_risk'] == 1]\n        critical_cases = pd.concat([critical_cases, stillbirths])\n    critical_cases = critical_cases.drop_duplicates()\n    \n    remaining_size = sample_size - len(critical_cases)\n    \n    if remaining_size > 0:\n        other_cases = df[~df.index.isin(critical_cases.index)]\n        sampled_others = other_cases.sample(n=remaining_size, random_state=42)\n        final_sample = pd.concat([critical_cases, sampled_others])\n    else:\n        final_sample = critical_cases.sample(n=sample_size, random_state=42)\n    \n    return final_sample\n\nsample_df=create_stratified_sample(df, 'stillbirth_risk')\n\nexclude_cols = [\n    # Targets & labels\n    'maternal_mortality_risk', 'stillbirth_risk', 'premature_birth_risk',\n    'birth_defect_risk', 'anc_dropout', 'high_risk_pregnancy',\n\n    # Derived risk scores and flags\n    'total_risk_factors', 'clinical_risk_score', 'risk_level', 'predicted_risk',\n    'total_missed_visits', 'age_risk_score', 'demographic_risk', 'anemia_risk_score',\n    'overall_risk_score',\n\n    # Identifiers\n    'MOTHER_ID', 'ANC_ID', 'CHILD_ID', 'unique_id', 'EID', 'UID_NUMBER',\n\n    # Delivery outcome or post-delivery info (leakage)\n    'WEIGHT_child_mean', 'DELIVERY_MODE', 'MATERNAL_OUTCOME', 'IS_DELIVERED',\n    'DELIVERY_OUTCOME', 'DATE_OF_DELIVERY', 'PLACE_OF_DELIVERY', 'DEL_TIME',\n    'DATE_OF_DISCHARGE', 'DISCHARGE_TIME', 'JSY_BENEFICIARY',\n    'IS_MOTHER_ALIVE', 'IS_CHILD_DEATH', 'CHILD_DEATH_DATE', 'CHILD_DEATH_REASON',\n    'DEFECT_HEALTH_CENTER', 'IS_DEFECTIVE_BIRTH', 'BIRTH_DEFECT_TYPE',\n    'BIRTH_DEFECT_SUBTYPE', 'DEFECT_SUBTYPE_OTHER', 'DEFECT_TYPE_OTHER',\n    'NOTIFICATION_SENT', 'FBIR_COMPLETED_BY_ANM', 'NEWBORN_SCREENING',\n    'DEATH_REASON_OTHER', 'SNCU_ADMITTED', 'SNCU_REFERRAL_HOSPITAL',\n    'TERTIARY_REFERRAL_HOSPITAL', 'OTHER_REFERRAL_HOSPITAL',\n    'DATE_OF_DEATH', 'REASON_FOR_DEATH', 'PLACE_OF_DEATH',\n    'INDICATION_FOR_C_SECTION', 'CH', 'CAH', 'GALACTOCEMIA', 'G6PDD', 'BIOTINIDASE',\n\n    # Delivery-specific process info\n    'DELIVERY_INSTITUTION', 'DELIVERY_DONE_BY', 'CONDUCT_BY',\n    'MISOPROSTAL_TABLET', 'DEL_COMPLICATIONS', 'OTHER_DEL_COMPLICATIONS',\n    'NOTIFICATION_SENT_del', 'FBIR_COMPLETED_BY_ANM_del',\n\n    # Administrative / logging columns\n    'REGISTRATION_DT', 'REGTYPE', 'CURRENT_USR', 'OTHER_STATE_PLACE',\n    'OTHER_STATE_PLACE_FILEPATH', 'OTHER_GOVT_PLACE_FILEPATH',\n    'ANC2_TAG_FAC_ID', 'ANC3_TAG_FAC_ID',\n\n    # Facility and geographic info\n    'ANC_INSTITUTE', 'FACILITY_TYPE', 'FACILITY_NAME', 'DOCTOR_ANM',\n    'DISTRICT_anc', 'DISTRICT_child',\n\n    # Feeding / newborn care\n    'IS_BF_IN_HOUR', 'FEEDING_TYPE', 'DATE_OF_FIRST_FEEDING',\n    'TIME_OF_FIRST_FEEDING', 'BABY_ON_MEDICATION', 'MEDICATION_REMARKS',\n    'DATE_OF_BLOODSAMPLE_COLLECTION', 'TIME_OF_BLOODSAMPLE_COLLECTION',\n    'HOURS_OF_SAMPLE_COLLECTION', 'TRANSFUSION_DONE',\n\n    # Screening/test results\n    'VDRL_DATE', 'VDRL_STATUS', 'HIV_DATE', 'HIV_STATUS', 'HBSAG_DATE',\n    'HBSAG_STATUS', 'HEP_DATE', 'HEP_STATUS', 'VDRL_RESULT',\n    'HIV_RESULT', 'HBSAG_RESULT', 'HEP_RESULT',\n\n    # Missed ANC flag columns\n    'MISSANC1FLG', 'MISSANC2FLG', 'MISSANC3FLG', 'MISSANC4FLG',\n\n    # Manually added known leaky or post-hoc columns\n    'HIGH_RISKS', 'DISEASES', 'CHILD_NAME',\n\n    # Any column with 'risk' or 'score' in name not already excluded\n    *[col for col in df.columns if (\n        ('risk' in col.lower() or 'score' in col.lower()) and col not in {\n            'age_risk_score', 'anemia_risk_score', 'overall_risk_score',\n            'demographic_risk', 'clinical_risk_score', 'total_risk_factors',\n            'maternal_mortality_risk', 'stillbirth_risk', 'premature_birth_risk',\n            'birth_defect_risk', 'high_risk_pregnancy', 'mental_health_risk'\n        }\n    )],\n]\n\nmore_leakage_cols = [\n    # aggregate or binary risk flags\n#     'bp_risk', 'mental_health_risk', 'age_category', 'multigravida', 'grand_multipara',\n#      'no_anc',\n#     'missed_first_anc', 'consecutive_missed',\n#     # anemia + BP severity flags\n#     'severe_hypertension',\n#     # weight / BMI flags\n#     'low_birth_weight', 'very_low_birth_weight', 'avg_birth_weight_low',\n#     # mental-health flags\n#     # trend or helper columns\n#     'hemoglobin_trend', 'unique_id', 'TT_DATE', 'MAL_PRESENT','IS_ADMITTED_SNCU',\n#  'IS_PREV_PREG','CHILD_ID', 'ANC1FLG',\n#  'ANC2FLG',\n#  'ANC3FLG',\n#  'ANC4FLG',\n#  'ANC_DATE',\n#  'DEATH',\n#  'EXP_DOD',\n#  'DELIVERY_PLACE',\n#  'EXP_DOD_preg',\n#  'FASTING',\n#  'LMP_DT',\n#  'SCREENED_FOR_MENTAL_HEALTH',\n#  'AGE',\n#  'AGE_final',\n#  'AGE_preg',\n#  'WEIGHT_child_min',\n# 'GENDER',\n#     'TIME_OF_BIRTH',\n#      'TWIN_PREGNANCY_max', 'TOTAL_ANC_VISITS',\n#     'RNK', \n\n#     # any other “risk / score” not already in exclude list\n#     *[c for c in df.columns if ('risk' in c.lower() or 'score' in c.lower()) and c not in exclude_cols]\n    \n    \"stillbirth_risk\",\n    \"DELIVERY_OUTCOME\",\n    \"IS_DEFECTIVE_BIRTH\",\n    \"BIRTH_DEFECT_TYPE\",\n    \"BIRTH_DEFECT_SUBTYPE\",\n    \"DEFECT_TYPE_OTHER\",\n    \"CHILD_DEATH_DATE\",\n    \"CHILD_DEATH_REASON\",\n    \"IS_CHILD_DEATH\",\n    \"DEATH_REASON_OTHER\",\n    \"MATERNAL_OUTCOME\",\n    \"REASON_FOR_DEATH\",\n    \"DATE_OF_DEATH\",\n    \"DATE_OF_DELIVERY\",\n    \"PLACE_OF_DELIVERY\",\n    \"MODE_OF_DELIVERY\",\n    \"INDICATION_FOR_C_SECTION\",\n    \"DEL_COMPLICATIONS\",\n    \"OTHER_DEL_COMPLICATIONS\",\n    \"NOTIFICATION_SENT_del\",\n    \"FBIR_COMPLETED_BY_ANM_del\",\n    \"IS_DELIVERED\",\n    \"DATE_OF_DISCHARGE\",\n    \"DISCHARGE_TIME\",\n    \"CHILD_ID\",\n    \"CHILD_NAME\",\n    \"GENDER\",\n    \"TIME_OF_BIRTH\",\n    \"IS_BF_IN_HOUR\",\n    \"FEEDING_TYPE\",\n    \"DATE_OF_FIRST_FEEDING\",\n    \"TIME_OF_FIRST_FEEDING\",\n    \"WEIGHT_child_mean\",\n    \"WEIGHT_child_min\",\n    \"avg_birth_weight_low\",\n    \"LOW_BIRTH_WEIGHT\",\n    \"very_low_birth_weight\",\n    \"IS_ADMITTED_SNCU\",\n    \"SNCU_ADMITTED\",\n    \"SNCU_REFERRAL_HOSPITAL\",\n    \"TERTIARY_REFERRAL_HOSPITAL\",\n    \"OTHER_REFERRAL_HOSPITAL\",\n    \"IMMUNE_CYCLE_DONE\",\n    \"BIRTH_SCREENING\",\n    \"NEWBORN_SCREENING\",\n    \"EID\",\n    \"DATE_OF_BLOODSAMPLE_COLLECTION\",\n    \"TIME_OF_BLOODSAMPLE_COLLECTION\",\n    \"HOURS_OF_SAMPLE_COLLECTION\",\n    \"TRANSFUSION_DONE\",\n    \"BABY_ON_MEDICATION\",\n    \"MEDICATION_REMARKS\",\n    \"CH\",\n    \"CAH\",\n    \"GALACTOCEMIA\",\n    \"G6PDD\",\n    \"BIOTINIDASE\",\n    \"total_risk_factors\",\n    \"high_risk_pregnancy\",\n    \"overall_risk_score\",\n    \"clinical_risk_score\",\n    \"demographic_risk\",\n    \"maternal_mortality_risk\",\n    \"premature_birth_risk\",\n    \"anemia_risk_score\",\n    \"bp_risk\",\n    \"mental_health_risk\",\n    \"age_risk_score\",\n    \"PHQ_SCORE_max\",\n    \"GAD_SCORE_max\",\n    \"depression\",\n    \"severe_depression\",\n    \"anxiety\",\n    \"severe_anxiety\",\n    \"mental_health_risk\",\n    \"SCREENED_FOR_MENTAL_HEALTH\",\n    \"NO_OF_WEEKS_max\",\n    \"TWIN_PREGNANCY_max\",\n    \"DELIVERY_PLACE\",\n    \"DELIVERY_INSTITUTION\",\n    \"DELIVERY_DONE_BY\",\n    \"CONDUCT_BY\",\n    \"OTHER_NAME\",\n    \"JSY_BENEFICIARY\",\n    \"NOTIFICATION_SENT\",\n    \"FBIR_COMPLETED_BY_ANM\",\n    \"OTHER_STATE_PLACE\",\n    \"OTHER_STATE_PLACE_FILEPATH\",\n    \"OTHER_GOVT_PLACE_FILEPATH\",\n        'IS_DEFECTIVE_BIRTH',\n    'EID',\n    'UID_NUMBER',\n    'IS_MOTHER_ALIVE',\n    'WEIGHT_child_mean',\n    'WEIGHT_child_min',\n    'low_birth_weight',\n    'very_low_birth_weight',\n    'avg_birth_weight_low',\n    'AGE_final',\n    'premature_birth_risk',\n    'high_risk_pregnancy',\n    'clinical_risk_score',\n    'overall_risk_score',\n    'total_risk_factors'\n\n]\n# 'NO_OF_WEEKS_max','HEMOGLOBIN_min', 'HEMOGLOBIN_max', 'WEIGHT_anc_mean', 'WEIGHT_anc_min', 'WEIGHT_anc_max', \n    # 'previous_loss', 'recurrent_loss',\n# Finally combine everything\nexclude_cols = list(set(exclude_cols + more_leakage_cols))\n\nfeatures = [col for col in df.columns if col not in exclude_cols and df[col].dtype in ['float64', 'float32', 'int64', 'int32', 'int8']]\n\nif not features:\n    print(f\"Skipping {target_columns}: No valid features available.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T11:47:33.552016Z","iopub.execute_input":"2025-06-17T11:47:33.552933Z","iopub.status.idle":"2025-06-17T11:48:25.295426Z","shell.execute_reply.started":"2025-06-17T11:47:33.552903Z","shell.execute_reply":"2025-06-17T11:48:25.294639Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score, f1_score\nimport lightgbm as lgb\nimport numpy as np\nimport time\n\n# Assuming sample_df, features, and target_column are defined (correcting target_columns to target_column)\nX = sample_df[features]\ny = sample_df[target_columns]  # Changed from target_columns to target_column\n\n# Initial train-test split\nX_train_full, X_test, y_train_full, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Initialize k-fold cross-validation\nn_splits = 5  # Number of folds\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Lists to store metrics for each fold\nauc_scores = []\nf1_scores = []\nfold_times = []\n\n# Calculate scale_pos_weight for class imbalance (based on training dataset)\nneg_count = (y_train_full == 0).sum()\npos_count = (y_train_full == 1).sum()\nscale_pos_weight = neg_count[0] / pos_count[0] if pos_count[0] > 0 else 1  # Simplified, assuming y is a Series\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T10:03:31.114315Z","iopub.execute_input":"2025-06-17T10:03:31.114682Z","iopub.status.idle":"2025-06-17T10:03:39.793515Z","shell.execute_reply.started":"2025-06-17T10:03:31.114653Z","shell.execute_reply":"2025-06-17T10:03:39.792335Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip show scikit-learn\n!pip show imbalanced-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T10:03:44.256925Z","iopub.execute_input":"2025-06-17T10:03:44.257294Z","iopub.status.idle":"2025-06-17T10:03:50.478229Z","shell.execute_reply.started":"2025-06-17T10:03:44.257270Z","shell.execute_reply":"2025-06-17T10:03:50.476658Z"}},"outputs":[{"name":"stdout","text":"Name: scikit-learn\nVersion: 1.2.2\nSummary: A set of python modules for machine learning and data mining\nHome-page: http://scikit-learn.org\nAuthor: \nAuthor-email: \nLicense: new BSD\nLocation: /usr/local/lib/python3.11/dist-packages\nRequires: joblib, numpy, scipy, threadpoolctl\nRequired-by: bayesian-optimization, Boruta, category_encoders, cesium, eli5, fastai, hdbscan, hep_ml, imbalanced-learn, librosa, lime, mlxtend, nilearn, pyLDAvis, pynndescent, rgf-python, scikit-learn-intelex, scikit-optimize, scikit-plot, sentence-transformers, shap, sklearn-compat, sklearn-pandas, TPOT, tsfresh, umap-learn, woodwork, yellowbrick\nName: imbalanced-learn\nVersion: 0.13.0\nSummary: Toolbox for imbalanced dataset in machine learning\nHome-page: https://imbalanced-learn.org/\nAuthor: \nAuthor-email: \"G. Lemaitre\" <g.lemaitre58@gmail.com>, \"C. Aridas\" <ichkoar@gmail.com>\nLicense: \nLocation: /usr/local/lib/python3.11/dist-packages\nRequires: joblib, numpy, scikit-learn, scipy, sklearn-compat, threadpoolctl\nRequired-by: \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install scikit-learn==1.2.2 imbalanced-learn==0.10.1 --no-deps -q\nfrom imblearn.over_sampling import SMOTE\nprint(\"SMOTE imported successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T10:03:53.683563Z","iopub.execute_input":"2025-06-17T10:03:53.684034Z","iopub.status.idle":"2025-06-17T10:03:57.571273Z","shell.execute_reply.started":"2025-06-17T10:03:53.683994Z","shell.execute_reply":"2025-06-17T10:03:57.570009Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hSMOTE imported successfully\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore', category=UserWarning)  # Suppress warnings for cleaner output\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, precision_recall_curve\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import SMOTE\nimport numpy as np\nimport time\nimport shap\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Verify SMOTE availability\ntry:\n    from imblearn.over_sampling import SMOTE\n    SMOTE_AVAILABLE = True\n    print(\"SMOTE imported successfully\")\nexcept ImportError:\n    SMOTE_AVAILABLE = False\n    print(\"Error: SMOTE not available. Please install imbalanced-learn: pip install imbalanced-learn\")\n    raise ImportError(\"SMOTE is required for this script.\")\n\n# Define exclude_cols (adjust based on your dataset)\n# Stratified sampling function\ndef create_stratified_sample(df, target_column, sample_size=2000000, min_positive=100):\n    child_death_col = 'IS_CHILD_DEATH'\n    critical_cases = pd.DataFrame()\n    \n    if target_column not in df.columns:\n        raise ValueError(f\"Target column '{target_column}' not found in DataFrame. Available columns: {df.columns.tolist()}\")\n    \n    if child_death_col in df.columns:\n        child_deaths = df[df[child_death_col] == 1]\n        critical_cases = pd.concat([critical_cases, child_deaths])\n    if 'maternal_mortality_risk' in df.columns:\n        maternal_deaths = df[df['maternal_mortality_risk'] == 1]\n        critical_cases = pd.concat([critical_cases, maternal_deaths])\n    if 'stillbirth_risk' in df.columns and target_column == 'stillbirth_risk':\n        stillbirths = df[df['stillbirth_risk'] == 1]\n        critical_cases = pd.concat([critical_cases, stillbirths])\n    critical_cases = critical_cases.drop_duplicates()\n    \n    positive_cases = df[df[target_column] == 1]\n    print(f\"Found {len(positive_cases)} positive cases for {target_column}\")\n    if len(positive_cases) == 0:\n        raise ValueError(f\"No positive cases ({target_column}=1) found in the dataset. Cannot proceed with sampling.\")\n    if len(positive_cases) < min_positive:\n        print(f\"Warning: Only {len(positive_cases)} positive cases found. Oversampling to {min_positive}.\")\n        oversampled_positives = positive_cases.sample(n=min_positive, replace=True, random_state=42)\n        critical_cases = pd.concat([critical_cases, oversampled_positives]).drop_duplicates()\n    \n    remaining_size = sample_size - len(critical_cases)\n    \n    if remaining_size > 0:\n        other_cases = df[~df.index.isin(critical_cases.index)]\n        if len(other_cases) < remaining_size:\n            print(f\"Warning: Only {len(other_cases)} non-critical cases available. Adjusting sample size.\")\n            remaining_size = len(other_cases)\n        sampled_others = other_cases.sample(n=remaining_size, random_state=42)\n        final_sample = pd.concat([critical_cases, sampled_others])\n    else:\n        final_sample = critical_cases.sample(n=sample_size, random_state=42)\n    \n    return final_sample\n\n# Diagnostic checks\nprint(\"Columns in df:\", df.columns.tolist())\nprint(\"stillbirth_risk distribution:\")\nprint(df['stillbirth_risk'].value_counts(dropna=False))\nprint(\"IS_CHILD_DEATH distribution:\")\nprint(df['IS_CHILD_DEATH'].value_counts(dropna=False))\nprint(\"DELIVERY_OUTCOME distribution:\")\nprint(df['DELIVERY_OUTCOME'].value_counts(dropna=False))\nprint(\"maternal_mortality_risk distribution:\")\nprint(df['maternal_mortality_risk'].value_counts(dropna=False))\n\n# Attempt to derive stillbirth_risk\nif df['stillbirth_risk'].eq(1).sum() == 0 and 'IS_CHILD_DEATH' in df.columns:\n    print(\"No positive cases in stillbirth_risk. Attempting to derive from IS_CHILD_DEATH...\")\n    df['stillbirth_risk'] = df['IS_CHILD_DEATH'].fillna(0).astype(int)\n    print(\"Redefined stillbirth_risk distribution:\")\n    print(df['stillbirth_risk'].value_counts(dropna=False))\n\n# Select target\ntarget_column = 'stillbirth_risk' if df['stillbirth_risk'].eq(1).sum() > 0 else 'maternal_mortality_risk'\nif df[target_column].eq(1).sum() == 0:\n    raise ValueError(f\"No positive cases found for {target_column}. Cannot proceed.\")\nprint(f\"Using target: {target_column}\")\n\n# Select numeric features\nfeatures = [col for col in df.columns if col not in exclude_cols and \n            df[col].dtype in ['float64', 'float32', 'int64', 'int32', 'int8']]\n\n# Create sample\ntry:\n    sample_df = create_stratified_sample(df, target_column, min_positive=100)\n    print(f\"Class distribution in sample_df for {target_column}:\")\n    print(sample_df[target_column].value_counts())\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n    raise\n\n# Check features\nif not features:\n    raise ValueError(f\"No valid features available for {target_column}. Available columns: {df.columns.tolist()}\")\nprint(\"Features used:\", features)\n\n# Prepare data\nX = sample_df[features]\ny = sample_df[target_column]\nX_train_full, X_test, y_train_full, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Initialize k-fold cross-validation\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Metrics storage with lower thresholds\nmetrics = {\n    'thresh_0_3': {'f1': [], 'accuracy': [], 'precision': [], 'recall': []},\n    'thresh_0_4': {'f1': [], 'accuracy': [], 'precision': [], 'recall': []},\n    'thresh_0_5': {'f1': [], 'accuracy': [], 'precision': [], 'recall': []},\n    'thresh_0_6': {'f1': [], 'accuracy': [], 'precision': [], 'recall': []},\n    'auc': []\n}\nfold_models = []\nfold_avg_f1 = []\n\n# Random Forest parameters optimized for recall\nparams = {\n    'n_estimators': 200,          # Increased for better learning\n    'max_depth': 15,              # Increased for more complexity\n    'min_samples_split': 20,      # Reduced for flexibility\n    'min_samples_leaf': 10,       # Reduced for flexibility\n    'class_weight': {0: 1, 1: 10}, # Heavily weight positive class\n    'random_state': 42,\n    'n_jobs': -1\n}\n\n# K-fold cross-validation\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X_train_full, y_train_full)):\n    print(f\"Training fold {fold + 1}/{n_splits}\")\n    \n    X_train, X_val = X_train_full.iloc[train_idx], X_train_full.iloc[val_idx]\n    y_train, y_val = y_train_full.iloc[train_idx], y_train_full.iloc[val_idx]\n    \n    print(f\"Training class counts:\\n{y_train.value_counts()}\")\n    print(f\"Validation class counts:\\n{y_val.value_counts()}\")\n    \n    if len(y_train.unique()) < 2 or len(y_val.unique()) < 2:\n        print(f\"Warning: Fold {fold + 1} has only one class. Skipping.\")\n        continue\n    \n    # Force SMOTE in every fold\n    print(f\"Forcing SMOTE for fold {fold + 1}\")\n    smote = SMOTE(random_state=42, sampling_strategy=0.5)  # 1:2 positive:negative ratio\n    X_train, y_train = smote.fit_resample(X_train, y_train)\n    print(f\"Post-SMOTE training class counts:\\n{y_train.value_counts()}\")\n    \n    start_time = time.time()\n    model = RandomForestClassifier(**params)\n    model.fit(X_train, y_train)\n    fold_time = time.time() - start_time\n    fold_models.append(model)\n    \n    y_pred_proba = model.predict_proba(X_val)\n    if y_pred_proba.shape[1] == 1:\n        print(f\"Warning: Fold {fold + 1} predicts only one class. Assigning zero probabilities for class 1.\")\n        y_pred_proba = np.zeros(len(X_val))\n    else:\n        y_pred_proba = y_pred_proba[:, 1]\n    \n    # Evaluate at lower thresholds\n    y_pred_0_3 = (y_pred_proba > 0.3).astype(int)\n    y_pred_0_4 = (y_pred_proba > 0.4).astype(int)\n    y_pred_0_5 = (y_pred_proba > 0.5).astype(int)\n    y_pred_0_6 = (y_pred_proba > 0.6).astype(int)\n    \n    f1_0_3 = f1_score(y_val, y_pred_0_3)\n    accuracy_0_3 = accuracy_score(y_val, y_pred_0_3)\n    precision_0_3 = precision_score(y_val, y_pred_0_3, zero_division=0)\n    recall_0_3 = recall_score(y_val, y_pred_0_3, zero_division=0)\n    cm_0_3 = confusion_matrix(y_val, y_pred_0_3)\n    \n    f1_0_4 = f1_score(y_val, y_pred_0_4)\n    accuracy_0_4 = accuracy_score(y_val, y_pred_0_4)\n    precision_0_4 = precision_score(y_val, y_pred_0_4, zero_division=0)\n    recall_0_4 = recall_score(y_val, y_pred_0_4, zero_division=0)\n    cm_0_4 = confusion_matrix(y_val, y_pred_0_4)\n    \n    f1_0_5 = f1_score(y_val, y_pred_0_5)\n    accuracy_0_5 = accuracy_score(y_val, y_pred_0_5)\n    precision_0_5 = precision_score(y_val, y_pred_0_5, zero_division=0)\n    recall_0_5 = recall_score(y_val, y_pred_0_5, zero_division=0)\n    cm_0_5 = confusion_matrix(y_val, y_pred_0_5)\n    \n    f1_0_6 = f1_score(y_val, y_pred_0_6)\n    accuracy_0_6 = accuracy_score(y_val, y_pred_0_6)\n    precision_0_6 = precision_score(y_val, y_pred_0_6, zero_division=0)\n    recall_0_6 = recall_score(y_val, y_pred_0_6, zero_division=0)\n    cm_0_6 = confusion_matrix(y_val, y_pred_0_6)\n    \n    auc = roc_auc_score(y_val, y_pred_proba) if len(np.unique(y_val)) > 1 else 0\n    \n    metrics['thresh_0_3']['f1'].append(f1_0_3)\n    metrics['thresh_0_3']['accuracy'].append(accuracy_0_3)\n    metrics['thresh_0_3']['precision'].append(precision_0_3)\n    metrics['thresh_0_3']['recall'].append(recall_0_3)\n    \n    metrics['thresh_0_4']['f1'].append(f1_0_4)\n    metrics['thresh_0_4']['accuracy'].append(accuracy_0_4)\n    metrics['thresh_0_4']['precision'].append(precision_0_4)\n    metrics['thresh_0_4']['recall'].append(recall_0_4)\n    \n    metrics['thresh_0_5']['f1'].append(f1_0_5)\n    metrics['thresh_0_5']['accuracy'].append(accuracy_0_5)\n    metrics['thresh_0_5']['precision'].append(precision_0_5)\n    metrics['thresh_0_5']['recall'].append(recall_0_5)\n    \n    metrics['thresh_0_6']['f1'].append(f1_0_6)\n    metrics['thresh_0_6']['accuracy'].append(accuracy_0_6)\n    metrics['thresh_0_6']['precision'].append(precision_0_6)\n    metrics['thresh_0_6']['recall'].append(recall_0_6)\n    \n    metrics['auc'].append(auc)\n    \n    avg_f1 = np.mean([f1_0_3, f1_0_4, f1_0_5, f1_0_6])\n    fold_avg_f1.append(avg_f1)\n    \n    print(f\"Fold {fold + 1}\")\n    print(f\"  AUC: {auc:.4f}, Time: {fold_time:.2f} seconds\")\n    print(f\"  Threshold 0.3 - F1: {f1_0_3:.4f}, Accuracy: {accuracy_0_3:.4f}, Precision: {precision_0_3:.4f}, Recall: {recall_0_3:.4f}\")\n    print(f\"  Confusion Matrix (Threshold 0.3):\\n{cm_0_3}\")\n    print(f\"  Threshold 0.4 - F1: {f1_0_4:.4f}, Accuracy: {accuracy_0_4:.4f}, Precision: {precision_0_4:.4f}, Recall: {recall_0_4:.4f}\")\n    print(f\"  Confusion Matrix (Threshold 0.4):\\n{cm_0_4}\")\n    print(f\"  Threshold 0.5 - F1: {f1_0_5:.4f}, Accuracy: {accuracy_0_5:.4f}, Precision: {precision_0_5:.4f}, Recall: {recall_0_5:.4f}\")\n    print(f\"  Confusion Matrix (Threshold 0.5):\\n{cm_0_5}\")\n    print(f\"  Threshold 0.6 - F1: {f1_0_6:.4f}, Accuracy: {accuracy_0_6:.4f}, Precision: {precision_0_6:.4f}, Recall: {recall_0_6:.4f}\")\n    print(f\"  Confusion Matrix (Threshold 0.6):\\n{cm_0_6}\")\n    \n    if len(np.unique(y_val)) > 1:\n        precisions, recalls, thresholds = precision_recall_curve(y_val, y_pred_proba)\n        f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n        optimal_threshold = thresholds[np.argmax(f1_scores)]\n        print(f\"  Optimal threshold: {optimal_threshold:.4f}\")\n\nprint(f\"\\nCross-Validation Mean Metrics:\")\nprint(f\"  AUC: {np.mean(metrics['auc']):.4f} ± {np.std(metrics['auc']):.4f}\")\nfor thresh in ['thresh_0_3', 'thresh_0_4', 'thresh_0_5', 'thresh_0_6']:\n    print(f\"\\n{thresh.replace('_', ' ').title()}:\")\n    print(f\"  F1 Score: {np.mean(metrics[thresh]['f1']):.4f} ± {np.std(metrics[thresh]['f1']):.4f}\")\n    print(f\"  Accuracy: {np.mean(metrics[thresh]['accuracy']):.4f} ± {np.std(metrics[thresh]['accuracy']):.4f}\")\n    print(f\"  Precision: {np.mean(metrics[thresh]['precision']):.4f} ± {np.std(metrics[thresh]['precision']):.4f}\")\n    print(f\"  Recall: {np.mean(metrics[thresh]['recall']):.4f} ± {np.std(metrics[thresh]['recall']):.4f}\")\n\nif fold_avg_f1:\n    best_fold_idx = np.argmax(fold_avg_f1)\n    best_model = fold_models[best_fold_idx]\n    print(f\"\\nBest Model from Fold {best_fold_idx + 1} with Average F1 Score: {fold_avg_f1[best_fold_idx]:.4f}\")\n    \n    y_test_pred_proba = best_model.predict_proba(X_test)\n    if y_test_pred_proba.shape[1] == 1:\n        print(\"Warning: Test set prediction has only one class. Assigning zero probabilities for class 1.\")\n        y_test_pred_proba = np.zeros(len(X_test))\n    else:\n        y_test_pred_proba = y_test_pred_proba[:, 1]\n    \n    y_test_pred_0_3 = (y_test_pred_proba > 0.3).astype(int)\n    y_test_pred_0_4 = (y_test_pred_proba > 0.4).astype(int)\n    y_test_pred_0_5 = (y_test_pred_proba > 0.5).astype(int)\n    y_test_pred_0_6 = (y_test_pred_proba > 0.6).astype(int)\n    \n    test_auc = roc_auc_score(y_test, y_test_pred_proba) if len(np.unique(y_test)) > 1 else 0\n    test_f1_0_3 = f1_score(y_test, y_test_pred_0_3)\n    test_accuracy_0_3 = accuracy_score(y_test, y_test_pred_0_3)\n    test_precision_0_3 = precision_score(y_test, y_test_pred_0_3, zero_division=0)\n    test_recall_0_3 = recall_score(y_test, y_test_pred_0_3, zero_division=0)\n    test_cm_0_3 = confusion_matrix(y_test, y_test_pred_0_3)\n    \n    test_f1_0_4 = f1_score(y_test, y_test_pred_0_4)\n    test_accuracy_0_4 = accuracy_score(y_test, y_test_pred_0_4)\n    test_precision_0_4 = precision_score(y_test, y_test_pred_0_4, zero_division=0)\n    test_recall_0_4 = recall_score(y_test, y_test_pred_0_4, zero_division=0)\n    test_cm_0_4 = confusion_matrix(y_test, y_test_pred_0_4)\n    \n    test_f1_0_5 = f1_score(y_test, y_test_pred_0_5)\n    test_accuracy_0_5 = accuracy_score(y_test, y_test_pred_0_5)\n    test_precision_0_5 = precision_score(y_test, y_test_pred_0_5, zero_division=0)\n    test_recall_0_5 = recall_score(y_test, y_test_pred_0_5, zero_division=0)\n    test_cm_0_5 = confusion_matrix(y_test, y_test_pred_0_5)\n    \n    test_f1_0_6 = f1_score(y_test, y_test_pred_0_6)\n    test_accuracy_0_6 = accuracy_score(y_test, y_test_pred_0_6)\n    test_precision_0_6 = precision_score(y_test, y_test_pred_0_6, zero_division=0)\n    test_recall_0_6 = recall_score(y_test, y_test_pred_0_6, zero_division=0)\n    test_cm_0_6 = confusion_matrix(y_test, y_test_pred_0_6)\n    \n    print(f\"\\nTest Set Metrics (Best Model from Fold {best_fold_idx + 1}):\")\n    print(f\"  AUC: {test_auc:.4f}\")\n    print(f\"\\nThreshold 0.3:\\n  F1: {test_f1_0_3:.4f}, Accuracy: {test_accuracy_0_3:.4f}, Precision: {test_precision_0_3:.4f}, Recall: {test_recall_0_3:.4f}\\n  Confusion Matrix:\\n{test_cm_0_3}\")\n    print(f\"\\nThreshold 0.4:\\n  F1: {test_f1_0_4:.4f}, Accuracy: {test_accuracy_0_4:.4f}, Precision: {test_precision_0_4:.4f}, Recall: {test_recall_0_4:.4f}\\n  Confusion Matrix:\\n{test_cm_0_4}\")\n    print(f\"\\nThreshold 0.5:\\n  F1: {test_f1_0_5:.4f}, Accuracy: {test_accuracy_0_5:.4f}, Precision: {test_precision_0_5:.4f}, Recall: {test_recall_0_5:.4f}\\n  Confusion Matrix:\\n{test_cm_0_5}\")\n    print(f\"\\nThreshold 0.6:\\n  F1: {test_f1_0_6:.4f}, Accuracy: {test_accuracy_0_6:.4f}, Precision: {test_precision_0_6:.4f}, Recall: {test_recall_0_6:.4f}\\n  Confusion Matrix:\\n{test_cm_0_6}\")\n    \n    test_f1_scores = {0.3: test_f1_0_3, 0.4: test_f1_0_4, 0.5: test_f1_0_5, 0.6: test_f1_0_6}\n    best_threshold = max(test_f1_scores, key=test_f1_scores.get)\n    print(f\"\\nBest Threshold on Test Set: {best_threshold} with F1 Score: {test_f1_scores[best_threshold]:.4f}\")\n    \n    # SHAP analysis with sampled test set\n    print(\"\\nPerforming SHAP analysis...\")\n    X_test_sample = X_test.sample(n=min(1000, len(X_test)), random_state=42)  # Sample to reduce computation\n    explainer = shap.TreeExplainer(best_model)\n    shap_values = explainer.shap_values(X_test_sample)\n    \n    plt.figure()\n    shap.summary_plot(shap_values[1], X_test_sample, show=False)\n    plt.savefig(\"shap_summary_plot.png\")\n    plt.close()\n    print(\"SHAP summary plot saved as 'shap_summary_plot.png'\")\n    \n    plt.figure()\n    shap.summary_plot(shap_values[1], X_test_sample, plot_type=\"bar\", show=False)\n    plt.savefig(\"shap_importance_bar.png\")\n    plt.close()\n    print(\"SHAP feature importance bar plot saved as 'shap_importance_bar.png'\")\n    \n    shap_importance = np.abs(shap_values[1]).mean(axis=0)\n    importance_df = pd.DataFrame({\n        'Feature': X_test_sample.columns,\n        'SHAP_Importance': shap_importance\n    }).sort_values(by='SHAP_Importance', ascending=False)\n    print(\"\\nSHAP Feature Importance:\")\n    print(importance_df)\n\nelse:\n    print(\"\\nNo valid models trained due to single-class folds.\")\n#     print(\"\\nPerforming SHAP analysis...\")\n#     explainer = shap.TreeExplainer(best_model)\n#     shap_values = explainer.shap_values(X_test)\n    \n#     plt.figure()\n#     shap.summary_plot(shap_values[1], X_test, show=False)\n#     plt.savefig(\"shap_summary_plot.png\")\n#     plt.close()\n#     print(\"SHAP summary plot saved as 'shap_summary_plot.png'\")\n    \n#     plt.figure()\n#     shap.summary_plot(shap_values[1], X_test, plot_type=\"bar\", show=False)\n#     plt.savefig(\"shap_importance_bar.png\")\n#     plt.close()\n#     print(\"SHAP feature importance bar plot saved as 'shap_importance_bar.png'\")\n    \n#     shap_importance = np.abs(shap_values[1]).mean(axis=0)\n#     importance_df = pd.DataFrame({\n#         'Feature': X_test.columns,\n#         'SHAP_Importance': shap_importance\n#     }).sort_values(by='SHAP_Importance', ascending=False)\n#     print(\"\\nSHAP Feature Importance:\")\n#     print(importance_df)\n# else:\n#     print(\"\\nNo valid models trained due to single-class folds.\")#     print(\"\\nPerforming SHAP analysis...\")\n#     explainer = shap.TreeExplainer(best_model)\n#     shap_values = explainer.shap_values(X_test)\n    \n#     plt.figure()\n#     shap.summary_plot(shap_values[1], X_test, show=False)\n#     plt.savefig(\"shap_summary_plot.png\")\n#     plt.close()\n#     print(\"SHAP summary plot saved as 'shap_summary_plot.png'\")\n    \n#     plt.figure()\n#     shap.summary_plot(shap_values[1], X_test, plot_type=\"bar\", show=False)\n#     plt.savefig(\"shap_importance_bar.png\")\n#     plt.close()\n#     print(\"SHAP feature importance bar plot saved as 'shap_importance_bar.png'\")\n    \n#     shap_importance = np.abs(shap_values[1]).mean(axis=0)\n#     importance_df = pd.DataFrame({\n#         'Feature': X_test.columns,\n#         'SHAP_Importance': shap_importance\n#     }).sort_values(by='SHAP_Importance', ascending=False)\n#     print(\"\\nSHAP Feature Importance:\")\n#     print(importance_df)\n# else:\n#     print(\"\\nNo valid models trained due to single-class folds.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T10:04:07.074160Z","iopub.execute_input":"2025-06-17T10:04:07.074591Z","iopub.status.idle":"2025-06-17T10:53:30.852826Z","shell.execute_reply.started":"2025-06-17T10:04:07.074529Z","shell.execute_reply":"2025-06-17T10:53:30.851023Z"}},"outputs":[{"name":"stdout","text":"SMOTE imported successfully\nColumns in df: ['ANC_ID', 'MOTHER_ID', 'GRAVIDA', 'ANC_DATE', 'ANC_INSTITUTE', 'FACILITY_TYPE', 'FACILITY_NAME', 'DOCTOR_ANM', 'BLOOD_SUGAR', 'IFA_TABLET', 'TT_GIVEN', 'USG_SCAN', 'MAL_PRESENT', 'PLACENTA', 'HIV', 'THYROID', 'RH_NEGATIVE', 'SYS_DISEASE', 'DISTRICT_anc', 'EXP_DOD', 'ANC_TYPE', 'URINE_TEST', 'URINE_SUGAR', 'URINE_ALBUMIN', 'UTERUS_SIZE', 'HEART_RATE', 'FOETAL_POSITION', 'FASTING', 'TT_DATE', 'POST_PRANDIAL', 'BLOOD_SUGAR_TEST', 'IRON_SUCROSE_INJ', 'SCREENED_FOR_MENTAL_HEALTH', 'PARA_TO_WARNING_SIGNS_HTN', 'RNK', 'unique_id', 'CHILD_ID', 'GENDER', 'TIME_OF_BIRTH', 'IS_DEFECTIVE_BIRTH', 'IS_BF_IN_HOUR', 'AGE', 'DELIVERY_PLACE', 'DELIVERY_MODE', 'IS_PREV_PREG', 'IS_ADMITTED_SNCU', 'SNCU_REFERRAL_HOSPITAL', 'TERTIARY_REFERRAL_HOSPITAL', 'OTHER_REFERRAL_HOSPITAL', 'CHILD_DEATH_DATE', 'CHILD_DEATH_REASON', 'IMMUNE_CYCLE_DONE', 'DISTRICT_child', 'DEFECT_HEALTH_CENTER', 'IS_CHILD_DEATH', 'BIRTH_DEFECT_TYPE', 'BIRTH_DEFECT_SUBTYPE', 'DEFECT_SUBTYPE_OTHER', 'NOTIFICATION_SENT', 'FBIR_COMPLETED_BY_ANM', 'CHILD_NAME', 'BIRTH_SCREENING', 'DEFECT_TYPE_OTHER', 'DEATH_REASON_OTHER', 'NEWBORN_SCREENING', 'EID', 'UID_NUMBER', 'SNCU_ADMITTED', 'CONSANGUINITY', 'HIGH_RISKS', 'DISEASES', 'FEEDING_TYPE', 'DATE_OF_FIRST_FEEDING', 'TIME_OF_FIRST_FEEDING', 'DATE_OF_BLOODSAMPLE_COLLECTION', 'TIME_OF_BLOODSAMPLE_COLLECTION', 'HOURS_OF_SAMPLE_COLLECTION', 'TRANSFUSION_DONE', 'BABY_ON_MEDICATION', 'CH', 'CAH', 'GALACTOCEMIA', 'G6PDD', 'BIOTINIDASE', 'MEDICATION_REMARKS', 'DATE_OF_DELIVERY', 'PLACE_OF_DELIVERY', 'DELIVERY_OUTCOME', 'MODE_OF_DELIVERY', 'MATERNAL_OUTCOME', 'REASON_FOR_DEATH', 'DATE_OF_DEATH', 'PLACE_OF_DEATH', 'INDICATION_FOR_C_SECTION', 'DELIVERY_INSTITUTION', 'DELIVERY_DONE_BY', 'IS_DELIVERED', 'DATE_OF_DISCHARGE', 'IS_MOTHER_ALIVE', 'DEL_TIME', 'MISOPROSTAL_TABLET', 'CONDUCT_BY', 'OTHER_NAME', 'JSY_BENEFICIARY', 'DISCHARGE_TIME', 'DEL_COMPLICATIONS', 'OTHER_DEL_COMPLICATIONS', 'NOTIFICATION_SENT_del', 'FBIR_COMPLETED_BY_ANM_del', 'OTHER_STATE_PLACE', 'OTHER_STATE_PLACE_FILEPATH', 'OTHER_GOVT_PLACE_FILEPATH', 'REGISTRATION_DT', 'LMP_DT', 'EXP_DOD_preg', 'PARITY', 'ABORTIONS', 'LIVE', 'DEATH', 'OBSTETRIC_FORMULA', 'AGE_preg', 'HEIGHT', 'BLOOD_GRP', 'ANC1FLG', 'ANC2FLG', 'ANC3FLG', 'ANC4FLG', 'MISSANC1FLG', 'MISSANC2FLG', 'MISSANC3FLG', 'MISSANC4FLG', 'REGTYPE', 'CURRENT_USR', 'ANC2_TAG_FAC_DIST', 'ANC2_TAG_FAC_ID', 'ANC3_TAG_FAC_DIST', 'ANC3_TAG_FAC_ID', 'VDRL_DATE', 'VDRL_STATUS', 'VDRL_RESULT', 'HIV_DATE', 'HIV_STATUS', 'HIV_RESULT', 'HBSAG_DATE', 'HBSAG_STATUS', 'HBSAG_RESULT', 'HEP_DATE', 'HEP_STATUS', 'HEP_RESULT', 'HEMOGLOBIN_mean', 'HEMOGLOBIN_min', 'HEMOGLOBIN_max', 'WEIGHT_anc_mean', 'WEIGHT_anc_min', 'WEIGHT_anc_max', 'BP_last', 'NO_OF_WEEKS_max', 'TWIN_PREGNANCY_max', 'PHQ_SCORE_max', 'GAD_SCORE_max', 'TOTAL_ANC_VISITS', 'WEIGHT_child_mean', 'WEIGHT_child_min', 'AGE_final', 'age_adolescent', 'age_elderly', 'age_very_young', 'age_risk_score', 'age_category', 'multigravida', 'grand_multipara', 'previous_loss', 'recurrent_loss', 'gravida_parity_ratio', 'inadequate_anc', 'no_anc', 'total_missed_visits', 'irregular_anc', 'missed_first_anc', 'consecutive_missed', 'anemia_mild', 'anemia_moderate', 'anemia_severe', 'ever_severe_anemia', 'anemia_risk_score', 'hemoglobin_trend', 'systolic_bp', 'diastolic_bp', 'hypertension', 'severe_hypertension', 'bp_risk', 'BMI', 'underweight', 'obese', 'normal_weight', 'depression', 'severe_depression', 'anxiety', 'severe_anxiety', 'mental_health_risk', 'weight_gain', 'weight_gain_per_week', 'inadequate_weight_gain', 'low_birth_weight', 'very_low_birth_weight', 'avg_birth_weight_low', 'demographic_risk', 'clinical_risk_score', 'overall_risk_score', 'maternal_mortality_risk', 'stillbirth_risk', 'premature_birth_risk', 'anc_dropout', 'total_risk_factors', 'high_risk_pregnancy']\nstillbirth_risk distribution:\nstillbirth_risk\n0    4029571\nName: count, dtype: int64\nIS_CHILD_DEATH distribution:\nIS_CHILD_DEATH\n0.0    3980904\n1.0      48667\nName: count, dtype: int64\nDELIVERY_OUTCOME distribution:\nDELIVERY_OUTCOME\nlive             4023217\nnone                3361\nstill birth         1832\ntwin birth           845\niud                  303\nmore than two         12\n-1                     1\nName: count, dtype: int64\nmaternal_mortality_risk distribution:\nmaternal_mortality_risk\n0    4028194\n1       1377\nName: count, dtype: int64\nNo positive cases in stillbirth_risk. Attempting to derive from IS_CHILD_DEATH...\nRedefined stillbirth_risk distribution:\nstillbirth_risk\n0    3980904\n1      48667\nName: count, dtype: int64\nUsing target: stillbirth_risk\nFound 48667 positive cases for stillbirth_risk\nClass distribution in sample_df for stillbirth_risk:\nstillbirth_risk\n0    1951333\n1      48667\nName: count, dtype: int64\nFeatures used: ['GRAVIDA', 'RNK', 'AGE', 'PARITY', 'ABORTIONS', 'AGE_preg', 'HEIGHT', 'HEMOGLOBIN_mean', 'HEMOGLOBIN_min', 'HEMOGLOBIN_max', 'WEIGHT_anc_mean', 'WEIGHT_anc_min', 'WEIGHT_anc_max', 'TOTAL_ANC_VISITS', 'age_adolescent', 'age_elderly', 'age_very_young', 'multigravida', 'grand_multipara', 'previous_loss', 'recurrent_loss', 'gravida_parity_ratio', 'inadequate_anc', 'no_anc', 'irregular_anc', 'missed_first_anc', 'consecutive_missed', 'anemia_mild', 'anemia_moderate', 'anemia_severe', 'ever_severe_anemia', 'hemoglobin_trend', 'systolic_bp', 'diastolic_bp', 'hypertension', 'severe_hypertension', 'BMI', 'underweight', 'obese', 'normal_weight', 'weight_gain', 'weight_gain_per_week', 'inadequate_weight_gain']\nTraining fold 1/5\nTraining class counts:\nstillbirth_risk\n0    1248852\n1      31148\nName: count, dtype: int64\nValidation class counts:\nstillbirth_risk\n0    312214\n1      7786\nName: count, dtype: int64\nForcing SMOTE for fold 1\nPost-SMOTE training class counts:\nstillbirth_risk\n0    1248852\n1     624426\nName: count, dtype: int64\nFold 1\n  AUC: 0.5845, Time: 489.67 seconds\n  Threshold 0.3 - F1: 0.0492, Accuracy: 0.1084, Precision: 0.0253, Recall: 0.9489\n  Confusion Matrix (Threshold 0.3):\n[[ 27299 284915]\n [   398   7388]]\n  Threshold 0.4 - F1: 0.0507, Accuracy: 0.2039, Precision: 0.0261, Recall: 0.8736\n  Confusion Matrix (Threshold 0.4):\n[[ 58447 253767]\n [   984   6802]]\n  Threshold 0.5 - F1: 0.0534, Accuracy: 0.3567, Precision: 0.0277, Recall: 0.7451\n  Confusion Matrix (Threshold 0.5):\n[[108332 203882]\n [  1985   5801]]\n  Threshold 0.6 - F1: 0.0572, Accuracy: 0.5388, Precision: 0.0301, Recall: 0.5745\n  Confusion Matrix (Threshold 0.6):\n[[167939 144275]\n [  3313   4473]]\n  Optimal threshold: 0.8075\nTraining fold 2/5\nTraining class counts:\nstillbirth_risk\n0    1248853\n1      31147\nName: count, dtype: int64\nValidation class counts:\nstillbirth_risk\n0    312213\n1      7787\nName: count, dtype: int64\nForcing SMOTE for fold 2\nPost-SMOTE training class counts:\nstillbirth_risk\n0    1248853\n1     624426\nName: count, dtype: int64\nFold 2\n  AUC: 0.5861, Time: 457.83 seconds\n  Threshold 0.3 - F1: 0.0489, Accuracy: 0.1024, Precision: 0.0251, Recall: 0.9486\n  Confusion Matrix (Threshold 0.3):\n[[ 25393 286820]\n [   400   7387]]\n  Threshold 0.4 - F1: 0.0508, Accuracy: 0.2063, Precision: 0.0262, Recall: 0.8734\n  Confusion Matrix (Threshold 0.4):\n[[ 59212 253001]\n [   986   6801]]\n  Threshold 0.5 - F1: 0.0535, Accuracy: 0.3580, Precision: 0.0277, Recall: 0.7453\n  Confusion Matrix (Threshold 0.5):\n[[108761 203452]\n [  1983   5804]]\n  Threshold 0.6 - F1: 0.0571, Accuracy: 0.5411, Precision: 0.0301, Recall: 0.5715\n  Confusion Matrix (Threshold 0.6):\n[[168717 143496]\n [  3337   4450]]\n  Optimal threshold: 0.7969\nTraining fold 3/5\nTraining class counts:\nstillbirth_risk\n0    1248853\n1      31147\nName: count, dtype: int64\nValidation class counts:\nstillbirth_risk\n0    312213\n1      7787\nName: count, dtype: int64\nForcing SMOTE for fold 3\nPost-SMOTE training class counts:\nstillbirth_risk\n0    1248853\n1     624426\nName: count, dtype: int64\nFold 3\n  AUC: 0.5901, Time: 424.57 seconds\n  Threshold 0.3 - F1: 0.0491, Accuracy: 0.1114, Precision: 0.0252, Recall: 0.9427\n  Confusion Matrix (Threshold 0.3):\n[[ 28296 283917]\n [   446   7341]]\n  Threshold 0.4 - F1: 0.0506, Accuracy: 0.1933, Precision: 0.0260, Recall: 0.8828\n  Confusion Matrix (Threshold 0.4):\n[[ 54975 257238]\n [   913   6874]]\n  Threshold 0.5 - F1: 0.0530, Accuracy: 0.3285, Precision: 0.0274, Recall: 0.7714\n  Confusion Matrix (Threshold 0.5):\n[[ 99114 213099]\n [  1780   6007]]\n  Threshold 0.6 - F1: 0.0576, Accuracy: 0.5160, Precision: 0.0302, Recall: 0.6076\n  Confusion Matrix (Threshold 0.6):\n[[160399 151814]\n [  3056   4731]]\n  Optimal threshold: 0.7993\nTraining fold 4/5\nTraining class counts:\nstillbirth_risk\n0    1248853\n1      31147\nName: count, dtype: int64\nValidation class counts:\nstillbirth_risk\n0    312213\n1      7787\nName: count, dtype: int64\nForcing SMOTE for fold 4\nPost-SMOTE training class counts:\nstillbirth_risk\n0    1248853\n1     624426\nName: count, dtype: int64\nFold 4\n  AUC: 0.5871, Time: 460.99 seconds\n  Threshold 0.3 - F1: 0.0492, Accuracy: 0.1050, Precision: 0.0253, Recall: 0.9522\n  Confusion Matrix (Threshold 0.3):\n[[ 26197 286016]\n [   372   7415]]\n  Threshold 0.4 - F1: 0.0510, Accuracy: 0.1991, Precision: 0.0263, Recall: 0.8847\n  Confusion Matrix (Threshold 0.4):\n[[ 56812 255401]\n [   898   6889]]\n  Threshold 0.5 - F1: 0.0536, Accuracy: 0.3425, Precision: 0.0278, Recall: 0.7654\n  Confusion Matrix (Threshold 0.5):\n[[103639 208574]\n [  1827   5960]]\n  Threshold 0.6 - F1: 0.0568, Accuracy: 0.5238, Precision: 0.0298, Recall: 0.5891\n  Confusion Matrix (Threshold 0.6):\n[[163023 149190]\n [  3200   4587]]\n  Optimal threshold: 0.8156\nTraining fold 5/5\nTraining class counts:\nstillbirth_risk\n0    1248853\n1      31147\nName: count, dtype: int64\nValidation class counts:\nstillbirth_risk\n0    312213\n1      7787\nName: count, dtype: int64\nForcing SMOTE for fold 5\nPost-SMOTE training class counts:\nstillbirth_risk\n0    1248853\n1     624426\nName: count, dtype: int64\nFold 5\n  AUC: 0.5820, Time: 451.22 seconds\n  Threshold 0.3 - F1: 0.0491, Accuracy: 0.1162, Precision: 0.0252, Recall: 0.9378\n  Confusion Matrix (Threshold 0.3):\n[[ 29868 282345]\n [   484   7303]]\n  Threshold 0.4 - F1: 0.0505, Accuracy: 0.1971, Precision: 0.0260, Recall: 0.8775\n  Confusion Matrix (Threshold 0.4):\n[[ 56255 255958]\n [   954   6833]]\n  Threshold 0.5 - F1: 0.0521, Accuracy: 0.3243, Precision: 0.0270, Recall: 0.7636\n  Confusion Matrix (Threshold 0.5):\n[[ 97815 214398]\n [  1841   5946]]\n  Threshold 0.6 - F1: 0.0563, Accuracy: 0.5087, Precision: 0.0295, Recall: 0.6027\n  Confusion Matrix (Threshold 0.6):\n[[158077 154136]\n [  3094   4693]]\n  Optimal threshold: 0.8191\n\nCross-Validation Mean Metrics:\n  AUC: 0.5859 ± 0.0027\n\nThresh 0 3:\n  F1 Score: 0.0491 ± 0.0001\n  Accuracy: 0.1087 ± 0.0048\n  Precision: 0.0252 ± 0.0001\n  Recall: 0.9461 ± 0.0051\n\nThresh 0 4:\n  F1 Score: 0.0507 ± 0.0002\n  Accuracy: 0.1999 ± 0.0047\n  Precision: 0.0261 ± 0.0001\n  Recall: 0.8784 ± 0.0046\n\nThresh 0 5:\n  F1 Score: 0.0531 ± 0.0005\n  Accuracy: 0.3420 ± 0.0139\n  Precision: 0.0275 ± 0.0003\n  Recall: 0.7582 ± 0.0109\n\nThresh 0 6:\n  F1 Score: 0.0570 ± 0.0004\n  Accuracy: 0.5257 ± 0.0126\n  Precision: 0.0299 ± 0.0002\n  Recall: 0.5890 ± 0.0145\n\nBest Model from Fold 4 with Average F1 Score: 0.0527\n\nTest Set Metrics (Best Model from Fold 4):\n  AUC: 0.5850\n\nThreshold 0.3:\n  F1: 0.0491, Accuracy: 0.1051, Precision: 0.0252, Recall: 0.9488\n  Confusion Matrix:\n[[ 32808 357459]\n [   498   9235]]\n\nThreshold 0.4:\n  F1: 0.0506, Accuracy: 0.1992, Precision: 0.0261, Recall: 0.8771\n  Confusion Matrix:\n[[ 71146 319121]\n [  1196   8537]]\n\nThreshold 0.5:\n  F1: 0.0533, Accuracy: 0.3433, Precision: 0.0276, Recall: 0.7600\n  Confusion Matrix:\n[[129922 260345]\n [  2336   7397]]\n\nThreshold 0.6:\n  F1: 0.0568, Accuracy: 0.5257, Precision: 0.0298, Recall: 0.5869\n  Confusion Matrix:\n[[204580 185687]\n [  4021   5712]]\n\nBest Threshold on Test Set: 0.6 with F1 Score: 0.0568\n\nPerforming SHAP analysis...\nSHAP summary plot saved as 'shap_summary_plot.png'\nSHAP feature importance bar plot saved as 'shap_importance_bar.png'\n\nSHAP Feature Importance:\n                   Feature  SHAP_Importance\n7          HEMOGLOBIN_mean     3.946451e-02\n9           HEMOGLOBIN_max     3.696075e-02\n8           HEMOGLOBIN_min     3.651416e-02\n27             anemia_mild     3.280592e-02\n2                      AGE     3.191294e-02\n42  inadequate_weight_gain     2.327729e-02\n17            multigravida     1.826008e-02\n0                  GRAVIDA     1.645487e-02\n3                   PARITY     1.616220e-02\n5                 AGE_preg     1.425452e-02\n12          WEIGHT_anc_max     1.231471e-02\n25        missed_first_anc     1.122210e-02\n10         WEIGHT_anc_mean     1.108933e-02\n28         anemia_moderate     1.096127e-02\n11          WEIGHT_anc_min     1.022489e-02\n21    gravida_parity_ratio     8.511118e-03\n4                ABORTIONS     8.379036e-03\n6                   HEIGHT     7.887321e-03\n19           previous_loss     7.706103e-03\n36                     BMI     6.587229e-03\n24           irregular_anc     5.244879e-03\n26      consecutive_missed     3.247614e-03\n32             systolic_bp     2.287200e-03\n39           normal_weight     1.732212e-03\n14          age_adolescent     1.522947e-03\n16          age_very_young     1.411442e-03\n38                   obese     9.535545e-04\n20          recurrent_loss     6.901366e-04\n37             underweight     5.287230e-04\n30      ever_severe_anemia     1.664559e-05\n15             age_elderly     1.592877e-05\n29           anemia_severe     1.297424e-05\n34            hypertension     8.444795e-06\n13        TOTAL_ANC_VISITS     1.097529e-06\n35     severe_hypertension     9.561731e-07\n22          inadequate_anc     7.626574e-07\n18         grand_multipara     3.036736e-07\n33            diastolic_bp     2.581337e-07\n1                      RNK     0.000000e+00\n23                  no_anc     0.000000e+00\n31        hemoglobin_trend     0.000000e+00\n40             weight_gain     0.000000e+00\n41    weight_gain_per_week     0.000000e+00\n","output_type":"stream"}],"execution_count":6}]}